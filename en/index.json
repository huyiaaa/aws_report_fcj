[{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"The Internet of Things on AWS \u0026ndash; Official Blog Sharing a vision of a more connected world with AWS IoT by Rachel Burke-Smalley on 03 MAY 2023 in Amazon SageMaker, Automotive, AWS IoT Core, AWS IoT ExpressLink, AWS IoT FleetWise, AWS IoT Greengrass, AWS IoT SiteWise, AWS IoT TwinMaker, AWS RoboMaker, CPG, Energy (Oil \u0026amp; Gas), Internet of Things, Kinesis Video Streams, Manufacturing, Robotics, Thought Leadership\nAWS IoT VP, Yasser Alsaied, shares IoT strategy, commitment, and vision Introduction\nThe future of the Internet of Things (IoT) is a subject of constant discussion and speculation. With the implications for IoT hyperscalers, vendors, and customers, the landscape is shifting rapidly. To shed some light on this topic, I sat down with Yasser Alsaied, Vice President of IoT at Amazon Web Services (AWS). In this blog, we explore Yasser’s perspectives on the future of IoT technology, strategy, industry evolution, and how these factors will impact the IoT ecosystem.\nYasser, thank you for agreeing to share your insights with us today. Could you start by telling us a bit about your role at AWS and your experience in the IoT space? In total, I have about 32 years of experience in the technology and IoT industry. I joined AWS in 2021 as the Vice President of IoT. I lead the AWS IoT business, covering Robotics, Industrial, Automotive, Consumer, Public Sector, and Commercial segments. Our services are among the most highly connected IoT services globally and continue to grow in the areas of digital twins, smart cities, and connected vehicles.\nBefore joining AWS, I worked at Qualcomm as the Vice President of IoT. During my time at Qualcomm, I held several leadership positions, including the launch of Qualcomm’s first wireless local area network (WLAN) chip for mobile phones, the Qualcomm Innovation Center, and the Code Aurora Foundation to address legal and operational issues related to open-source software releases. In my last four years at Qualcomm, I managed their IoT ecosystem and led the strategy for expanding IoT chipsets, including key technologies such as artificial intelligence (AI), computer vision, drones, robotics, and 5G. Prior to Qualcomm, I developed software for military applications, working with the U.S. Space and Rocket Center, Raytheon, Lockheed Martin Missiles and Fire Control, and Coleman Aerospace.\nHaving been involved since the early days of IoT, what are your thoughts on the current changes in the IoT industry, and what does this mean for customers and service providers? As the IoT industry evolves, we continue to see very strong interest and adoption of IoT from customers. We do not anticipate IoT growth slowing down anytime soon. However, it is clear that the role of IoT hyperscalers is changing, and much of this is tied to the industry\u0026rsquo;s shift toward verticalized solutions rather than providing IoT as a horizontal offering.\nThis shift is in the best interest of the customer because they never come to us saying \u0026ldquo;we need IoT.\u0026rdquo; Instead, they come to us wanting to achieve specific business outcomes and need support in defining how they can leverage technology to monitor, control, and optimize business processes to achieve those results. What this shift entails for cloud providers and IoT software vendors is that they must place IoT within the context of distinct industry challenges by providing industry-focused solutions and partners to drive business value based on the customer\u0026rsquo;s internal capabilities, IT scaling requirements, and use cases.\nHow does this shift in roles affect AWS\u0026rsquo;s strategy?\nAt AWS, we have adopted an industry-led approach as the \u0026ldquo;North Star\u0026rdquo; of our IoT strategy. To better serve customers, we have verticalized our IoT and AI services to optimize outcomes for each segment. For example, [AWS IoT SiteWise] and [AWS IoT TwinMaker] are built to support Industrial IoT customers in collecting, organizing, and analyzing industrial data, as well as creating digital twins of real-world industrial environments. [AWS IoT FleetWise] supports Automotive and connected vehicle customers. We also collaborate with industry-focused partners to build solutions. Our principle is to re-architect IoT products, sales, and support to align with each industry, thereby meeting customer needs more effectively.\nWhy are vertical industry solutions and partners so important to AWS and its customers? Some customers, when starting with IoT, face difficulties in moving from proof of concept to production, scaling up, and integrating all the necessary services. There are too many components to combine in a workload, and customers don\u0026rsquo;t care which services are included or how they are integrated. Instead, they just want industry-focused solutions that are placed within the context of specific challenges and are easy to manage, adapt, expand, and replicate.\nTherefore, a key part of our IoT strategy is focusing on vertical AWS and Partner Solutions, providing ready-to-deploy code, configurations, and architectural guidance customizable for specific use cases. Our approach always starts with understanding the customer\u0026rsquo;s unique vision and challenges, such as improving yield or fleet management. From there, we work backward, designing a solution tailored to their specific needs.\nUsing our trusted and scalable cloud platform, with over 200 services including AI, machine learning, analytics, compute, networking, and storage, we enable the building of solutions that combine all necessary elements. When something new needs to be built on this platform, we leverage our partner network to create solutions suitable for individual customers, which can also benefit other customers in the same industry. This service formula can be replicated globally, accelerating the process.\nAnother key initiative of ours is to continue developing and collaborating with the [AWS Partner Network] (APN), because we understand that customer access to a diverse range of partners is critical to filling skill gaps, meeting scaling requirements, and specializing in vertical use cases. These are partners who can support various stages of the IoT design lifecycle. ISV partners provide pre-built software solutions that can be integrated on top of our platform and IoT services to enhance capabilities and features. When customers need customization to meet their exact requirements, they can work with a highly specialized SI to help integrate various systems and technologies into a unified solution without needing to invest in additional internal engineering resources.\nBeyond pursuing vertical solutions, what does the future of IoT at AWS look like? Where is AWS placing its big bets?\nThere has been a lot of speculation about what we will do next, and I can confirm that we remain strongly committed to continuously innovating and making it easier and more cost-effective for customers to realize IoT solutions. In the two years since I joined AWS, we have launched new specialized services such as AWS IoT FleetWise, [AWS IoT ExpressLink], and AWS IoT TwinMaker. Our inventory of qualified devices has grown to over [800 partner devices]. We have announced [price reductions] to drive cost efficiency and deployed over [50 IoT feature updates], including the newly announced [AWS IoT Core for Amazon Sidewalk], which enhances the [integration] between [AWS IoT Core] and Amazon Sidewalk. We will not stop. We will continue to invest heavily in IoT because it is an integral component of how we solve customer problems and help them scale. Use cases like Industry 4.0 require data to be managed and processed at scale, in a reliable and secure manner. The data is already there, but without IoT, it is locked in isolated factories and production floors. IoT facilitates connectivity and unlocks data, making smart factories a reality. IoT is an essential part of the digitalization story – whether it\u0026rsquo;s 5G, Wi-Fi 7, LPWAN, or other technologies.\nFurthermore, our commitment to making IoT solutions more accessible is directly tied to our ongoing efforts to expand edge computing and hybrid cloud capabilities. By extending AWS infrastructure, services, APIs, and tools to edge locations like on-premises data centers, 5G towers, and smart factories, we can bring all the benefits of the cloud to workloads requiring low latency, local data residency, local data processing, or complex dependencies between applications. Today, AWS offers IoT services like [AWS IoT SiteWise Edge] and [AWS IoT Greengrass]{.underline} that act as edge gateways, making it easy to collect, process, and monitor device data on-site while bringing intelligence to edge devices. AWS is also developing functions to make deploying applications at the edge easier, shortening the time to value for customers and shifting capital expenditure (CapEx) to operating expenditure (OpEx).\nWhy is IoT important to Amazon\u0026rsquo;s success?\nMany of Amazon\u0026rsquo;s core business areas, such as warehousing operations, fulfillment centers, transportation, logistics, stores, and devices, are directly supported by AWS IoT services. We are also our own customer — and we are always customer-obsessed — so the service must be the best. Through our vast customer base, including Amazon, this technology is proven in reliability and scalability. We are stronger together and driven by core operational needs as well as Amazon\u0026rsquo;s DNA of innovation.\nFor example, IoT plays a critical role in the retail logistics chain and shopping process. Globally, Amazon has over 300 fulfillment centers operated by IoT and edge devices such as over 750,000 autonomous robotic units. This scale supports 1.6 million packages passing through their fulfillment centers every day. [Automation and robotics are a big part of this equation], and these technologies require real-time control, monitoring, and maintenance to keep Amazon\u0026rsquo;s supply chain running like clockwork. With AWS IoT, Amazon can collect and process telemetry data that helps operators monitor their activities, track critical maintenance schedules, and ensure all packages are delivered on time.\nAmazon also operates a large fleet of trucks for pickups across the country. previously, at many Amazon Fulfillment Centers, drivers had to wait in long lines to check in and find out which dock they were assigned to. But with AWS IoT, and specifically the specialized service [Amazon Kinesis Video Streams], we streamlined the check-in and check-out process. Now, drivers can automatically check in and see where they need to go right on their mobile app. This saved 775,000 hours for drivers and carriers across North America in 2022 alone. Additionally, Amazon has the Amazon Flex app, which coordinates field deliveries for Amazon delivery personnel. Flex sends and receives data to dozens of backend services via AWS IoT, processing up to 22,000 transactions per second to coordinate deliveries. Finally, when you shop at physical retail stores, many Amazon and third-party stores use Just Walk Out technology to skip the checkout line. To enter the store, customers can use the Amazon Shopping App, Amazon One, or a credit card, simply take the products they want, and leave, making shopping faster and more convenient. I have experienced this myself and I am very impressed with the convenience. Just Walk Out uses Kinesis Video Streams to transmit video from in-store cameras to AWS, running advanced computer vision algorithms that allow shoppers to enter the store, grab what they want, and leave.\nBeing our own customer also brings its own challenges. The massive scale at which Amazon operates globally means AWS IoT must support devices, telemetry data streams, and security at the same scale. Our internal deployments continue to yield new lessons that help the service become more robust, efficient, and cost-effective for customers to benefit from.\nWhich industries are benefiting from AWS IoT? Over the past decade, IoT has evolved from an aspirational technology to a core differentiator that businesses are using to solve problems and create value through revenue growth, operational improvements, and innovation. For consumers, we partner with many Original Equipment Manufacturers (OEMs) and Original Design Manufacturers (ODMs) with connected devices. They can quickly build secure products, connect devices, and support scalability and reliability. One example is Rachio, which uses AWS IoT to securely connect its devices to cloud applications and other devices. AWS IoT also gave [Rachio] a seamless gateway to other AWS services, such as [AWS Elastic Beanstalk] for deploying and managing their website, web apps, and API infrastructure. Most recently, Rachio began using the Amazon Alexa Skills Kit to enable voice control for the second version of the Rachio Smart Sprinkler Controller. With AWS IoT security capabilities, Rachio reduced development costs by 40% and no longer has to worry about managing availability and scalability.\nIn industrial and manufacturing, we partner with customers to improve process efficiency and yield, track inventory, manage operations, and safety. One example is our customer, [Coca-Cola İçecek (CCI)], who used AWS IoT to digitize their factory floor and production processes to create a complete digital twin solution scalable to all 26 bottling plants. Within two months, they built an advanced digital analytics solution for their line sanitation process using AWS IoT SiteWise and AWS IoT Greengrass. Through this, they improved process efficiency and environmental sustainability, allowing for savings of 20% in energy and 9% in water annually. In the public sector, smart cities, and transportation, customers are using IoT to improve operations, logistics, tactical edge support, and traffic management as well as public safety.\nIn commercial and smart buildings, customers are looking to improve retail experiences and enhance building sustainability. Finally, in Robotics, customers are using IoT services to collect data from robots to build new applications, optimize automation, and improve performance. For connected vehicles, IoT helps customers manage vehicle data, thereby improving machine learning models and in-cabin experiences. For example: [WirelessCar] is partnering with AWS IoT to manage their fleet at scale without managing infrastructure, allowing them to connect millions of vehicles to the cloud.\nWhat IoT trends should we expect as we move into the next phase? I believe IoT will become a common expectation in business rather than an exception. Many businesses and industries will continue to invest in IoT because it delivers business and operational value. We continuously see new customer segments, like the retail industry, unlocking the value of IoT. Customers are trying to build a unified experience that can move easily between online and offline, leading to a convergence with mobile, social, and IoT to serve wherever and whenever they desire. We also see increasing demand for simpler IoT-focused tools. It is important to make these tools more accessible so companies can innovate and leverage them, while easily penetrating industries as the adoption of IoT technologies continues to grow. For example, tools like [AWS IoT Core Device Advisor] help developers validate IoT devices for reliable and secure connectivity with AWS. They can identify device software issues, such as inability to reconnect, and get detailed logs to troubleshoot during development and testing.\nFinally, customers are expanding their sustainability initiatives, going beyond just reducing emissions to creating smart environments (e.g., cities, buildings, factories), leveraging IoT to monitor energy performance, minimize waste, and adjust facility operations based on actual usage trends. A great example of this is how Yara partnered with us to build an efficient and sustainable [Digital Production Platform] (DPP) for the agriculture industry. This DPP platform is a key element in digitizing their production system across 28 production sites, 122 production units, and two mines. The DPP detects, collects, and runs deep analytics on production data related to yield, reliability, environment, safety, quality, and innovation, using AWS IoT SiteWise, AWS IoT Greengrass, AWS IoT Core, [AWS IoT Analytics], and [Amazon SageMaker]. This solution helped Yara predict product quality and composition, improve utility balance at facilities, and detect when machines need repair or maintenance to maintain optimal production efficiency levels.\nYasser, thanks again for giving us an exclusive look at the future of IoT at AWS. Clearly, AWS remains deeply invested in IoT solutions to meet the unique needs of customers, including Amazon itself.\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"New and updated courses from AWS Training and Certification in March 2023 By Training and Certification Blog Editor, March 30, 2023\n(Topics: .NET, Amazon Aurora, Amazon CloudWatch, Amazon Detective, Amazon DynamoDB, Amazon Elastic Container Service, Amazon GuardDuty, Amazon RDS, Amazon Redshift, Amazon Security Lake, Amazon Textract, Amazon VPC, Analytics, Announcements, AWS IoT Device Management, AWS Lambda, AWS Secrets Manager, AWS Security Hub, AWS Training and Certification, Compute, Culture and Training, Customer Solutions, Database, Developer Tools, DevOps, Enterprise Strategy, Internet of Things, Mainframe Migration, Migration, Migration Solutions, MySQL compatible, Networking \u0026amp; Content Delivery, RDS for MySQL, Robotics, Security, Security, Identity, \u0026amp; Compliance)\nIn March 2023, AWS Training and Certification released 27 digital products, including 14 AWS Builder Labs, one AWS Jam Journey (analytics), two courses for AWS Partners, three courses for c-suite leaders, and one in-person classroom course on security best practices on AWS, to equip individuals with skills to work with AWS services and solutions.\nExam labs have been temporarily removed from the AWS Certified SysOps Administrator – Associate exam. To help organizations expand their ability to train their teams, the AWS Skill Builder Team subscription is now available in 50 countries. New for Learners AWS Skill Builder is our online learning center for all learners, offering over 600+ free courses, along with 125+ AWS Builder Labs, AWS Jam challenges, AWS Cloud Quest and AWS Industry Quest (experiential learning), and AWS Certification preparation resources, available with Individual and Team subscriptions.\nFree Digital Courses on AWS Skill Builder Fundamental Introduction to Robotics on AWS (30 minutes): Designed for technical teams using AWS for robotics development. In this course, you will learn how to use AWS to accelerate robot development and solve common challenges for robotics companies. Securely Connecting AWS IoT Devices to the Cloud (60 minutes): Designed for technical staff and teaches how to securely connect devices to the cloud using AWS IoT Core, ensuring the correct setup of AWS IoT policies. .NET Workloads on AWS App Runner (2 hours): Designed for developers learning how to write code and deploy containerized applications on AWS using C# and Visual Studio. You will learn how to build and test C# code in containers, store and deploy images to the AWS Cloud using AWS App Runner, and how to build and operate scalable containerized workloads on the AWS Cloud. .NET Workloads on AWS Lambda (3 hours): Designed for application developers new to AWS serverless compute, teaching you how to create, deploy, and manage AWS Lambda functions in current and future applications. Handling AWS IoT Device Data and States (2 hours): Designed for technical staff using AWS to handle Internet of Things (IoT) data. You will learn about telemetry and IoT communication protocols, as well as how to use IoT device data in the AWS Cloud. Intermediate Using AWS Solutions – AWS Cloud Migration Factory (90 minutes): Designed for technical individuals, such as solutions architects and server administrators. This course teaches the key features, use cases, components, and architecture of the AWS Cloud Migration Factory, helping you plan and execute large-scale server migrations. Advanced Planning Large-Scale Data Migrations to AWS (60 minutes): Designed for technical individuals, such as storage engineers and cloud architects. This course teaches key concepts, considerations, and requirements for performing large-scale data migrations to AWS, as well as how to create a data migration plan using the appropriate AWS storage service. Training for AWS Partners Intermediate AWS Partner: Mainframe Modernization Proof of Concept Scoping with AWS Blu Insights (Technical) (70 minutes): Designed for solutions architects and project managers at AWS Partner organizations who participate in evaluating mainframe refactoring projects. You will learn how to scope a Proof of Concept (POC) to transform legacy mainframe application source code and how to define POC boundaries using AWS Blu Insights. AWS Partner: AWS Mainframe Modernization Automated Refactor Transformation Center (Technical) (75 minutes): Designed for solutions architects at AWS Partner organizations who gather modernization requirements. You will learn how to transform and refactor legacy mainframe applications from COBOL to Java via AWS Blu Insights, and how to use the AWS Mainframe Modernization Automated Refactor Transformation Center to handle code errors, generate code refactoring, and deploy custom transformations. AWS Certification As of March 28, 2023, the AWS Certified SysOps Administrator – Associate exam will not include exam labs until further notice. This removal is temporary while we evaluate the exam labs and make improvements to provide the best experience for candidates. If you are preparing for this exam, please use the resources listed on the exam page. New Features for AWS Skill Builder Subscriptions Engage in the Best of re:Invent Analytics 2022: An AWS Jam Journey containing analytics challenges introduced at AWS re:Invent 2022, designed for anyone interested in leveraging analytics skills to complete hands-on challenges in a gamified format. This AWS Jam Journey joins other learning paths on Security, Networking, Machine Learning, Database, Games, and DevOps. AWS Builder Labs AWS Builder Labs are guided, self-paced, interactive learning environments in a live AWS Management Console. We have added 14 new labs to the collection of over 125 existing labs.\nFundamental Getting Started with Amazon Textract: Process Documents with Synchronous and Asynchronous Operations (60 minutes): Designed for developers, data engineers, and machine learning engineers. This lab guides you through navigating the pre-built AWS Cloud9 environment, reviewing AWS CDK project files and Python packages. You will test a synchronous process (extracting a 1-page document) and an asynchronous process (multi-page document). Build, Secure, and Monitor Networks on AWS (2 hours): Designed for DevOps and infrastructure engineers. You will help \u0026ldquo;AnyCompany\u0026rdquo; scale and secure their AWS network environment. Working with Amazon VPC Network Access Analyzer (60 minutes): Designed for system administrators. You will learn how to use Amazon VPC Network Access Analyzer to understand network configurations and identify unintended access. Walkthrough of the AWS Well-Architected Tool (30 minutes): Designed for solutions architects and cloud engineers. Teaches how to use the Well-Architected Tool to create/update a workload and review reports. Intermediate Zero Trust Architecture for Service-to-Service Workload (75 minutes): Designed for system administrators, teaching skills to deploy and verify security controls according to zero trust principles. Building with Amazon Redshift Clusters (60 minutes): Designed for solutions/database architects. Use the AWS Management Console and SQL Workbench to experiment with table layouts and schema designs. .NET Workloads on AWS Lambda (60 minutes): Designed for .NET developers. Learn how to deploy, edit, and invoke serverless .NET applications using the AWS Toolkit for Visual Studio. Migrating RDS MySQL to Aurora with Read Replica (60 minutes): Perform a data migration process from MySQL to Aurora with step-by-step guidance. Building Highly Available Web Application (2 hours): Learn how to set up a highly available three-tier web application using core AWS services. Building with Amazon RDS Databases (60 minutes): Learn how to set up Amazon RDS Multi-AZ failover with encryption and implement AWS Secrets Manager]. Building with Amazon Aurora Databases (60 minutes): Explore parallel query, how to scale with large datasets, and automated triggering scenarios. Building with Amazon DocumentDB Databases (60 minutes): Learn a method to convert a relational data model to non-relational and import data into Amazon DocumentDB. Building with Amazon DynamoDB Tables (60 minutes): Learn how to manage DynamoDB, run queries, backup, monitor, and use Amazon DAX clusters. Advanced Observing, Troubleshooting, and Optimizing Workloads Running on Amazon ECS (90 minutes): Designed for solutions architects and developers. Learn how to deploy, maintain, and troubleshoot applications on an Amazon ECS container cluster. New for Executives, Teams, and Organizations Free Digital Training, Fundamental Cloud for CFOs (13 minutes): Designed for Chief Financial Officers. Learn what cloud computing is, its role in transformation, challenges, and how to partner with CIOs. Cloud for CIOs (9 minutes): Designed for Chief Information Officers. Learn why you should move to the cloud, how to succeed, and how to get started. Digital Transformation for Executives (7 minutes): Designed for executive leaders. Learn about digital transformation, its importance, and how to implement it. AWS Classroom Training Security Engineering on AWS: An intermediate 3-day training, instructor-led. Designed for security engineers/architects and cloud architects. Includes 8 modules and 7 new hands-on labs covering Amazon Security Lake, AWS GuardDuty, Amazon Detective, and more. AWS Skill Builder Team Subscription – Global Expansion AWS Skill Builder Team subscription is now available in 50 countries. New countries include: Argentina, Austria, Bahrain, Belgium, Bulgaria, Chile, Cyprus, Czech Republic, Estonia, Finland, Iceland, Indonesia, Lithuania, Luxembourg, Malta, Mainland China, Mexico, Norway, Poland, Portugal, Romania, Slovakia, Slovenia, South Africa, Sri Lanka, Sweden, and Ukraine.\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Hannover Messe: Discover an end-to-end industrial data strategy with AWS by Scot Wlodarczak on 20 MAR 2023 in Agriculture, Amazon EC2, Amazon Lookout for Vision, Amazon Monitron, Amazon SageMaker, Amazon Textract, Automotive, AWS IoT Core, AWS IoT FleetWise, AWS IoT Greengrass, AWS IoT SiteWise, AWS Outposts, AWS RoboMaker, AWS Snow Family, CPG, Edge, End User Computing, Energy, Energy (Oil \u0026amp; Gas), Events, Hi-Tech and Electronics, Life Sciences, Manufacturing, Power \u0026amp; Utilities, Semiconductor, Supply Chain, Sustainability, Telecommunications, Thought Leadership\nVisit our booth The world’s largest industrial conference, Hannover Messe, begins on April 17, 2023 in Hannover, Germany. Once again, look for Amazon Web Services (AWS) to have a large presence at the event, but in a different location than previous years, in Hall 15 booth D74 right by the main entryway.\nTo get your free code to attend the event, check out our event website page here, or click here to setup a 1:1 meeting with an AWS expert.\nIndustrial Data Fabric (IDF) Data is the foundation of any digital transformation effort, and having an industrial data strategy is critical to enable business teams to easily and effectively leverage that data to address a variety of use cases across an organization.\nWhy? Manufacturers have struggled with disconnected and siloed data sources that were not designed to work together, making it challenging to solve business problems and make informed decisions with non-homogenous data. While they might find success with a single proof of concept on a specific use-case, they often find challenges with scaling implementation to other factories, other use cases, and different teams.\nAWS helps create a data management architecture that enables a scalable, unified, and integrated mechanism to harness data as an asset. We call this the Industrial Data Fabric (IDF).\nIDF Benefits: By providing economical, secure, structured, and easy access to high quality datasets across an entire organization, business leaders can build the foundation for digital industrial transformation and operational optimization across multiple scenarios beyond a single proof-of-concept such as in quality, maintenance, material management, and process optimization. IDF Solutions: Include Open Industrial Data Architecture that enables best design practices for storage and easy accessibility of industrial data, with interoperability between AWS Services and AWS Partner Solutions so manufacturers aren’t locked into proprietary data schemas or creating more data silos in the cloud. We implement IDF in multiple ways, with AWS Solutions, prescriptive solution guidance such as reference architectures, partner solutions like HighByte, Element, General Electric, Mendix Multi-App Platform Package, and AWS Services.\nAn example of Solution Guidance for Industrial Data Fabric (IDF) is the ‘Industrial Data’ AWS Well-Architected Lens with architectural guidance and best practices for AWS Services and partner solutions, enabling manufacturers to accelerate and scale the ingestion, contextualization, and action on industrial data as well as enterprise data across the entire value chain.\nCheck out some of the IDF solutions on the solution page at here. Come to our booth and ask how you can leverage your valuable operational technology (OT) data.\nConnect with Partners Speak with AWS experts at kiosks and demos to learn how cloud and AWS partner solutions help industrial companies in Engineering \u0026amp; Design, Smart Manufacturing, Supply Chain, Sustainability, Smart Products \u0026amp; Services to shorten time to market, improve efficiency, reduce costs, and increase revenue.\nSpeak with platinum sponsors in the booth, including Bosch, Siemens, and MHP, to learn how they can simplify your Industry 4.0 transformation, shorten time to value, and de-risk projects.\nBosch The Bosch Group is a leading global supplier of technology and services. Bosch’s operations are divided into four business sectors: Mobility Solutions, Industrial Technology, Consumer Goods, and Energy and Building Technology. As a leading IoT provider, Bosch offers innovative solutions for smart homes, Industry 4.0, and connected mobility. Bosch is pursuing a vision of mobility that is sustainable, safe, and exciting. Experience their demos and displays at the AWS booth. Siemens Siemens Digital Industries Software helps organizations digitally transform using software, hardware, and services from the Siemens Xcelerator business platform. Siemens’ software and digital twin enable companies to optimize their design, engineering, and manufacturing processes to turn today’s ideas into the sustainable products of the future. Mendix, a Siemens business and global leader in enterprise low-code, is fundamentally reinventing the way applications are built in the digital enterprise. Experience their demos and displays at the AWS booth. MHP As a technology and business partner, MHP digitizes its customers\u0026rsquo; processes and products, and guides them through IT transformations along the entire value chain. MHP is a digitalization pioneer in the mobility and manufacturing sectors, with expertise that can be transferred to a wide range of industries. MHP advises on both operational and strategic issues, offering proven IT and technology expertise as well as specific industry knowledge. Experience their demos and displays at the AWS booth. Other Partners Additionally, there is a range of other AWS Partners also participating in the AWS booth to solve multiple customer use cases. Gold sponsors in our booth include: 42Q, Accenture, ATOS, BCG, Denali, Matterport, along with silver sponsors: ABB, AllCloud, Autodesk, Azeti, Cyient, DXC, Element Analytics, GE Digital, Highbyte, Inductive Automation, Infosys, Mindcurv, Software AG, Software Defined Automation, Storm Reply, Syntax, TensorIT, Tulip, Wipro, and Zoi.\nSee our solutions in action Don’t miss the interactive demos at the booth.\nAWS IoT Partner Device Wall: Shows how partner devices can simplify data connectivity and accelerate digital transformation in the industry. Integrated Manufacturing Value Chain: Watch a demo illustrating how the Industrial Data Fabric is used to build a smart product (a car). From 3D CAD drawings and simulations in the Design \u0026amp; Engineering phase, to a smart factory demo in the Production and Asset Optimization area, and finally seeing the completed car on the track where it collects and visualizes telemetry data in real-time – you will see how AWS and partners collaborate to optimize operations. Sustainability: Dive deeper into the topic of sustainability with the Carbon Footprint solution that helps track your carbon footprint. Supply Chain: With today\u0026rsquo;s trend focusing on smart supply chains, be sure to find out how the AWS Supply Chain service can simplify supply chain needs for complex manufacturing operations. Discover new topics in the theater area The Theater area right in our booth will host more than 50 presentation sessions throughout the week, presented by AWS experts, AWS Partners, and customers. These sessions dive into smart manufacturing, quality management, and industrial automation, allowing you to research deeper and ask more technical questions.\nJoin these short presentations to learn new topics, ask questions, and connect with peers. See the detailed mini-theater schedule posted here and around the AWS booth every day.\nCome to the booth, speak with experts, and see how AWS and partners can help your organization build a data strategy to digitally transform faster, and reduce risk.\nGet a free event discount code to attend here, or click here to book a 1:1 meeting with an AWS expert.\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “Vietnam Cloud Day 2025: Ho Chi Minh Connect Edition for Builders” Event Objectives Share real-world examples of cloud adoption in enterprises. Highlight the practical business benefits enabled by cloud transformation. Explore best practices, tools, and innovations from AWS and industry leaders. Opening \u0026amp; Keynote Speakers Hon. Government Speaker Eric Yeo, Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Dr. Jens Lottner, CEO, Techcombank Ms. Trang Phung, CEO \u0026amp; Co-Founder, U2U Network Jaime Valles, Vice President, General Manager Asia Pacific and Japan, AWS Jeff Johnson, Managing Director, ASEAN, AWS Vu Van, Co-Founder \u0026amp; CEO, ELSA Corp Nguyen Hoa Binh, Chairman, Nexttech Group Dieter Botha, CEO, TymeX Track Sessions \u0026amp; Featured Speakers Completing Large-Scale Migration and Modernization with AWS\nSon Do, Technical Account Manager, AWS Nguyen Van Hai, Director of Software Engineering, Techcombank Modernizing Applications with Generative AI-Powered Tools\nPhuc Nguyen, Solutions Architect, AWS Alex Tran, AI Director, OCB Panel Discussion: Application Modernization – Accelerating Business Transformation\nModerator: Hung Nguyen Gia, Head of Solutions Architect, AWS Panelists: Nguyen Minh Ngan, AI Specialist, OCB Nguyen Manh Tuyen, Head of Data Application, LPBank Securities Vinh Nguyen, Co-Founder \u0026amp; CTO, Ninety Eight Transforming VMware with AI-driven Cloud Modernization\nHung Hoang, Customer Solutions Manager, AWS AWS Security at Scale: From Development to Production\nTaiki Dang, Solutions Architect, AWS Key Takeaways AWS continues to be a strong enabler of digital transformation across industries in Vietnam. Enterprises like Techcombank and OCB are actively leveraging AWS for modernization and AI-driven innovation. Generative AI (Amazon Q Developer) is reshaping the software development lifecycle by improving code quality, documentation, and security integration. VMware-to-AWS migration strategies provide organizations with scalable, secure, and cost-effective modernization pathways. Security remains a top priority, with AWS emphasizing “security by design” and AI-driven detection and remediation. Lessons Learned from Vietnam Cloud Day 2025 Large-Scale Migration and Modernization with AWS Techcombank’s Challenge: Managing the complexity of migrating mission-critical banking systems while ensuring service continuity. Solution: Adopted a phased migration strategy, leveraging AWS migration accelerators and cloud-native monitoring to minimize risks. AWS Best Practice: Start with a comprehensive readiness assessment to identify risks early and design mitigation strategies. Balancing Modernization and Stability: Combined phased migration with modernization to maintain both speed and reliability. Modernizing Applications with Generative AI-Powered Tools Amazon Q Developer for Legacy Systems: Automates code analysis, refactoring, test generation, and documentation for Java/.NET workloads. OCB Experience: Integrated AI to achieve faster release cycles and improved maintainability in banking apps. DevSecOps Integration: Embedded security and compliance validation directly into the development pipeline. Application Modernization Panel Insights Financial Institutions vs. Startups: Banks focus on compliance and stability, while startups prioritize speed and agility. Common Ground: Both seek scalability and innovation via cloud-native modernization. Success Factor: Technology is key, but organizational culture and adoption mindset drive success. Ninety Eight’s Advantage: Early modernization with cloud-native/serverless gave agility and faster innovation compared to larger organizations. Transforming VMware with AI-Driven Modernization Reducing Downtime: Used automation playbooks and downtime-aware patterns for smooth VMware-to-AWS migration. Post-Migration Priority: Focused on cost optimization by right-sizing and shifting to managed services. Compliance: For financial firms, compliance requirements shaped the migration roadmap, ensuring audit-ready security. AWS Security at Scale AI for Threat Detection: Leveraged AI-driven anomaly detection and automated incident response. Shift-Left Security vs. Speed: Embedded security checks early in CI/CD to achieve both agility and compliance. Common Pitfalls: Weak IAM practices, underestimating shared responsibility, and delaying security integration. Case-Specific Experiences OCB (Orient Commercial Bank): Applied Amazon Q Developer within DevSecOps, accelerating development while ensuring regulatory compliance and data protection. LPBank Securities: Chose event-driven architecture (Kafka/Kinesis) to meet low-latency trading needs over microservices-only models. Ninety Eight (Startup): Adopted cloud-native and serverless-first to balance scalability and cost. Although initial investment was high, cost efficiency improved significantly after fine-tuning workloads. Some event photos Overall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives (per CloudJourney roadmap) Complete account setup and security configuration (Module 1). Master basic networking concepts: VPC, Subnet, Route Table, Security Group (Module 2). Set up the working environment (Hugo) and record logs in Markdown. Complete the hands-on labs on https://cloudjourney.awsstudygroup.com/ to verify work. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 1 — Account setup \u0026amp; security: Create AWS account, enable root MFA, create user groups \u0026amp; basic policies, configure billing alerts and budget. 08/09/2025 08/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 1 Tuesday Module 2 — Networking theory: Study VPC, Subnet, Route Table, Security Group, ENI/EIP, ELB concepts. Prepare Hugo environment. 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 2 Wednesday Module 2 — Labs: Create a VPC, public/private subnets, launch EC2 in a public subnet, configure Security Groups, create a NAT Gateway, SSH into EC2. 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 2 Labs Thursday Module 2 — Labs continued: Set up VPC Peering, configure NACLs, cross-VPC route tables; adjust templates (optimize instance types). 11/09/2025 11/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 2 Labs Friday Module 3 — Storage overview \u0026amp; labs: EBS vs Instance Store, S3 static site hosting, S3 versioning \u0026amp; replication, Backup Plan \u0026amp; Restore; Storage Gateway lab. 12/09/2025 12/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 3 Results \u0026amp; Achievements (Week 1) AWS account created and secured: root MFA enabled, admin user \u0026amp; group created, budget and billing alerts configured. Completed Module 2 theory and gained clear understanding of VPC, Subnet, Route Table, Security Group, and common connection patterns (NAT, Peering, Transit Gateway concepts). Hands-on labs performed: EC2 launched and accessed via SSH, Security Groups configured, NAT Gateway and VPC Peering created, NACLs applied, and template issues resolved (adjusted instance types). Storage labs completed: S3 static hosting with versioning and replication; Backup Plan/Vault configured; Storage Gateway set up. Hugo site initialized for reporting and the weekly worklog recorded in Markdown; project files organized. Issues Encountered \u0026amp; Mitigations Module 1 budget lab (Lab 7-3) showed an empty usage-type dropdown → logged the issue, retried later, will escalate to lab support if it persists. Some lab templates lacked Security Group rules → added minimal required rules (SSH/HTTP) to proceed. Uncleaned resources caused minor unexpected charges → performed resource cleanup and enabled stricter billing alerts. Next Steps (Week 2) Continue Module 3: EFS/FSx and MGN labs; deepen storage and backup practice. Start Module 4: Compute (ECS/EKS/Lambda) and integrate with the established networking setup. Prepare a short guide for the team on resource cleanup and cost optimization. Collect screenshots, logs, and scripts in the project folder for submission. "},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Account Setup and Basic Networking (Modules 1 \u0026amp; 2)\nWeek 2: Advanced Storage and Compute Introduction (Modules 3 \u0026amp; 4)\nWeek 3: Advanced Compute and Databases (Module 4 \u0026amp; Databases)\nWeek 4: Expanded Databases and Monitoring (Databases \u0026amp; Monitoring)\nWeek 5: Server and Application Migration (Module 2)\nWeek 6: Database Migration and Hybrid Networking (Module 2)\nWeek 7: Cost Optimization and Performance (Module 3)\nWeek 8: Reliability and Advanced Security\nWeek 9: Microservices and Serverless\nWeek 10: Container Orchestration\nWeek 11: Data Lake and Analytics\nWeek 12: AI/ML Foundations\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/","title":"Internship Report","tags":[],"description":"","content":"Student Information: Full Name: Dương Viết Huy\nPhone Number: 0858901719\nEmail: huydvse184528@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Assurance\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"FastAPI Application Structure","tags":[],"description":"","content":"FastAPI Application Structure In this section, you will understand the FastAPI application structure and how it\u0026rsquo;s organized using clean layered architecture.\nApplication Architecture The FastAPI application follows a layered architecture pattern:\n┌─────────────────────────────────────────┐ │ API Layer (routers/) │ ← HTTP endpoints, request/response ├─────────────────────────────────────────┤ │ Service Layer (services/) │ ← Business logic, orchestration ├─────────────────────────────────────────┤ │ Repository Layer (repositories/) │ ← Data access, DynamoDB operations ├─────────────────────────────────────────┤ │ Models (models/) │ ← Data structures, validation └─────────────────────────────────────────┘ Main Application Entry Point # backend/app/main.py from fastapi import FastAPI from app.api.routers.auth import router as auth_router from app.api.routers.products import router as products_router from app.api.routers.orders import router as orders_router from app.core.logging import configure_logging from app.api.middleware import add_correlation_middleware def create_app() -\u0026gt; FastAPI: configure_logging() app = FastAPI(title=\u0026#34;Products \u0026amp; Orders API\u0026#34;, version=\u0026#34;1.0.0\u0026#34;) add_correlation_middleware(app) @app.get(\u0026#34;/health\u0026#34;, tags=[\u0026#34;health\u0026#34;]) async def health() -\u0026gt; dict: return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;} app.include_router(auth_router, prefix=\u0026#34;/auth\u0026#34;, tags=[\u0026#34;auth\u0026#34;]) app.include_router(products_router, prefix=\u0026#34;/products\u0026#34;, tags=[\u0026#34;products\u0026#34;]) app.include_router(orders_router, prefix=\u0026#34;/orders\u0026#34;, tags=[\u0026#34;orders\u0026#34;]) return app app = create_app() Lambda Handler # backend/app/lambda_handler.py from mangum import Mangum from app.main import app # Expose Lambda handler for API Gateway HTTP API handler = Mangum(app) How Mangum Works:\nConverts API Gateway HTTP API events to ASGI requests Routes requests to FastAPI application Converts FastAPI responses back to API Gateway format Configuration Management # backend/app/core/config.py from pydantic_settings import BaseSettings from pydantic import Field import os class Settings(BaseSettings): aws_region: str = Field(..., alias=\u0026#34;AWS_REGION\u0026#34;) products_table: str = Field(..., alias=\u0026#34;PRODUCTS_TABLE\u0026#34;) orders_table: str = Field(..., alias=\u0026#34;ORDERS_TABLE\u0026#34;) jwt_secret: str = Field(..., alias=\u0026#34;JWT_SECRET\u0026#34;) log_level: str = Field(\u0026#34;INFO\u0026#34;, alias=\u0026#34;LOG_LEVEL\u0026#34;) class Config: env_file = os.getenv(\u0026#34;ENV_FILE\u0026#34;, \u0026#34;.env\u0026#34;) case_sensitive = True settings = Settings() # will raise if required env missing Environment Variables:\nAWS_REGION: AWS region (e.g., us-east-1) PRODUCTS_TABLE: DynamoDB products table name ORDERS_TABLE: DynamoDB orders table name JWT_SECRET: JWT secret from Secrets Manager LOG_LEVEL: Logging level (default: INFO) API Routers Auth Router # backend/app/api/routers/auth.py from fastapi import APIRouter, Depends, HTTPException, status from pydantic import BaseModel, EmailStr from app.api.deps import get_auth_service from app.services.auth_service import AuthService router = APIRouter() class RegisterRequest(BaseModel): email: EmailStr password: str class LoginRequest(BaseModel): email: EmailStr password: str class TokenResponse(BaseModel): access_token: str token_type: str = \u0026#34;bearer\u0026#34; @router.post(\u0026#34;/register\u0026#34;, status_code=status.HTTP_201_CREATED) async def register(payload: RegisterRequest, auth_service: AuthService = Depends(get_auth_service)) -\u0026gt; dict: await auth_service.register_user(email=payload.email, password=payload.password) return {\u0026#34;message\u0026#34;: \u0026#34;registered\u0026#34;} @router.post(\u0026#34;/login\u0026#34;, response_model=TokenResponse) async def login(payload: LoginRequest, auth_service: AuthService = Depends(get_auth_service)) -\u0026gt; TokenResponse: token = await auth_service.login(email=payload.email, password=payload.password) return TokenResponse(access_token=token) Endpoints:\nPOST /auth/register - Register new user POST /auth/login - Login and get JWT token Products Router # backend/app/api/routers/products.py from fastapi import APIRouter, Depends, HTTPException, status, Query from typing import Optional, List from app.api.deps import require_admin, get_product_service from app.services.product_service import ProductService from app.models.product import ProductCreate, ProductUpdate, ProductOut router = APIRouter() @router.post(\u0026#34;\u0026#34;, response_model=ProductOut, status_code=status.HTTP_201_CREATED, dependencies=[Depends(require_admin)]) async def create_product(payload: ProductCreate, service: ProductService = Depends(get_product_service)) -\u0026gt; ProductOut: return await service.create_product(payload) @router.get(\u0026#34;\u0026#34;, response_model=List[ProductOut]) async def list_products(category: Optional[str] = Query(default=None), service: ProductService = Depends(get_product_service)) -\u0026gt; List[ProductOut]: return await service.list_products(category=category) @router.get(\u0026#34;/{product_id}\u0026#34;, response_model=ProductOut) async def get_product(product_id: str, service: ProductService = Depends(get_product_service)) -\u0026gt; ProductOut: product = await service.get_product(product_id) if not product: raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=\u0026#34;Product not found\u0026#34;) return product @router.put(\u0026#34;/{product_id}\u0026#34;, response_model=ProductOut, dependencies=[Depends(require_admin)]) async def update_product(product_id: str, payload: ProductUpdate, service: ProductService = Depends(get_product_service)) -\u0026gt; ProductOut: return await service.update_product(product_id, payload) @router.delete(\u0026#34;/{product_id}\u0026#34;, status_code=status.HTTP_204_NO_CONTENT, dependencies=[Depends(require_admin)]) async def delete_product(product_id: str, service: ProductService = Depends(get_product_service)) -\u0026gt; None: await service.delete_product(product_id) Endpoints:\nGET /products - List all products (optional category filter) GET /products/{product_id} - Get single product POST /products - Create product (admin only) PUT /products/{product_id} - Update product (admin only) DELETE /products/{product_id} - Delete product (admin only) Dependency Injection # backend/app/api/deps.py from fastapi import Depends, HTTPException, status from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer from typing import Optional from app.core.security import decode_jwt from app.core.config import settings from app.models.user import CurrentUser, Role from app.services.product_service import ProductService from app.repositories.ddb_products import DynamoProductsRepository security = HTTPBearer(auto_error=False) def get_products_repo() -\u0026gt; DynamoProductsRepository: return DynamoProductsRepository(settings.aws_region, settings.products_table) def get_product_service(repo: DynamoProductsRepository = Depends(get_products_repo)) -\u0026gt; ProductService: return ProductService(repo) def get_current_user(creds: Optional[HTTPAuthorizationCredentials] = Depends(security)) -\u0026gt; CurrentUser: if creds is None: raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\u0026#34;Missing credentials\u0026#34;) payload = decode_jwt(creds.credentials) sub = payload.get(\u0026#34;sub\u0026#34;) role = payload.get(\u0026#34;role\u0026#34;, Role.user.value) if not sub: raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\u0026#34;Invalid token\u0026#34;) return CurrentUser(user_id=sub, role=Role(role)) def require_admin(role: Role = Depends(get_current_user_role)) -\u0026gt; None: if role != Role.admin: raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail=\u0026#34;Admin only\u0026#34;) JWT Security Implementation # backend/app/core/security.py from datetime import datetime, timedelta, timezone from typing import Dict, Any from jose import jwt from passlib.context import CryptContext from app.core.config import settings ALGORITHM = \u0026#34;HS256\u0026#34; ACCESS_TOKEN_EXPIRE_MINUTES = 60 * 6 # 6 hours pwd_context = CryptContext(schemes=[\u0026#34;bcrypt\u0026#34;], deprecated=\u0026#34;auto\u0026#34;) def create_access_token(subject: str, role: str) -\u0026gt; str: expire = datetime.now(timezone.utc) + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES) to_encode: Dict[str, Any] = {\u0026#34;sub\u0026#34;: subject, \u0026#34;role\u0026#34;: role, \u0026#34;exp\u0026#34;: expire} return jwt.encode(to_encode, settings.jwt_secret, algorithm=ALGORITHM) def decode_jwt(token: str) -\u0026gt; Dict[str, Any]: return jwt.decode(token, settings.jwt_secret, algorithms=[ALGORITHM]) def verify_password(plain_password: str, hashed_password: str) -\u0026gt; bool: return pwd_context.verify(plain_password, hashed_password) def get_password_hash(password: str) -\u0026gt; str: return pwd_context.hash(password) Security Features:\nJWT tokens with 6-hour expiration Password hashing using bcrypt Role-based access control (admin/user) Token validation on every protected endpoint Correlation ID Middleware # backend/app/api/middleware.py import uuid import logging from starlette.middleware.base import BaseHTTPMiddleware from starlette.requests import Request logger = logging.getLogger(__name__) class CorrelationMiddleware(BaseHTTPMiddleware): async def dispatch(self, request: Request, call_next): correlation_id = request.headers.get(\u0026#34;X-Correlation-ID\u0026#34;, str(uuid.uuid4())) logger.info(f\u0026#34;Request started\u0026#34;, extra={ \u0026#34;correlation_id\u0026#34;: correlation_id, \u0026#34;method\u0026#34;: request.method, \u0026#34;path\u0026#34;: request.url.path }) response = await call_next(request) response.headers[\u0026#34;X-Correlation-ID\u0026#34;] = correlation_id logger.info(f\u0026#34;Request completed\u0026#34;, extra={ \u0026#34;correlation_id\u0026#34;: correlation_id, \u0026#34;status_code\u0026#34;: response.status_code }) return response def add_correlation_middleware(app): app.add_middleware(CorrelationMiddleware) Purpose:\nTrack requests across logs using correlation IDs Useful for debugging and tracing requests in distributed systems Dockerfile # backend/Dockerfile # Lambda base image for Python 3.10 FROM public.ecr.aws/lambda/python:3.10 # Copy and install requirements COPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt -t \u0026#34;${LAMBDA_TASK_ROOT}\u0026#34; # Copy application code COPY app/ ${LAMBDA_TASK_ROOT}/app/ # Set the handler CMD [\u0026#34;app.lambda_handler.handler\u0026#34;] Key Points:\nUses official AWS Lambda Python 3.10 base image Installs dependencies to ${LAMBDA_TASK_ROOT} Copies application code Sets handler to app.lambda_handler.handler Local Development To run locally:\n# Install dependencies make install # Run with uvicorn make run-local # or uvicorn backend.app.main:app --reload Access Swagger documentation at: http://localhost:8000/docs\nThe layered architecture (API → Service → Repository) makes the code testable, maintainable, and follows separation of concerns principles. Each layer has a single responsibility.\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/5-workshop/5.1-workshop-overview/","title":"Quick Start: DevSecOps with FastAPI Backend","tags":[],"description":"","content":"Quick Start: DevSecOps with FastAPI Backend This workshop guides you through quickly setting up a DevSecOps pipeline for a FastAPI Backend on AWS Lambda. Just configure a few basic things and the pipeline will automatically build, scan, and deploy.\nRepository: https://gitlab.com/m.quang/devsecops-aws-ver2\nQuick Start Checklist Just follow these 5 steps:\nConfigure AWS CLI - aws configure Create JWT Secret - aws secretsmanager create-secret Create ECR Repository - aws ecr create-repository Configure GitLab Variables - Add 7 variables in GitLab Push Code - git push and the pipeline will automatically run Time: ~15-20 minutes\nWhat You\u0026rsquo;ll Get? After completion, you\u0026rsquo;ll have:\nFastAPI Backend running on AWS Lambda (container) GitLab CI/CD Pipeline automatically: Semgrep - Security scan (SAST) Docker Build - Build container image Trivy - Vulnerability scan Terraform Deploy - Automatically deploy infrastructure API Gateway - REST API endpoint DynamoDB - 3 tables (products, orders, users) CloudWatch - Monitoring \u0026amp; alerting Detailed Steps Step 1: Clone \u0026amp; Configure AWS CLI # Clone repository git clone https://gitlab.com/m.quang/devsecops-aws-ver2.git cd devsecops-aws-ver2/Backend-FastAPI-Docker_Build-Pipeline # Configure AWS CLI aws configure # Enter: Access Key, Secret Key, Region (ap-southeast-1), Format (json) Step 2: Create JWT Secret JWT_SECRET=$(openssl rand -hex 32) aws secretsmanager create-secret \\ --name fastapi-jwt-secret \\ --secret-string \u0026#34;$JWT_SECRET\u0026#34; \\ --region ap-southeast-1 # Get ARN (save it) JWT_SECRET_ARN=$(aws secretsmanager describe-secret \\ --secret-id fastapi-jwt-secret \\ --region ap-southeast-1 \\ --query \u0026#39;ARN\u0026#39; \\ --output text) echo \u0026#34;JWT_SECRET_ARN: $JWT_SECRET_ARN\u0026#34; Step 3: Create ECR Repository aws ecr create-repository \\ --repository-name fastapi-lambda \\ --image-scanning-configuration scanOnPush=true \\ --region ap-southeast-1 # Get ECR URI (save it) ECR_URI=$(aws ecr describe-repositories \\ --repository-names fastapi-lambda \\ --region ap-southeast-1 \\ --query \u0026#39;repositories[0].repositoryUri\u0026#39; \\ --output text) echo \u0026#34;ECR_URI: $ECR_URI\u0026#34; Step 4: Configure GitLab CI/CD Variables Go to GitLab project → Settings → CI/CD → Variables → Expand\nAdd 7 variables:\nVariable Value Protected Masked AWS_ACCESS_KEY_ID Your AWS Access Key ✅ ❌ AWS_SECRET_ACCESS_KEY Your AWS Secret Key ✅ ✅ AWS_DEFAULT_REGION ap-southeast-1 ❌ ❌ JWT_SECRET_ARN arn:aws:secretsmanager:... ❌ ❌ ECR_URI 123456789012.dkr.ecr.ap-southeast-1.amazonaws.com/fastapi-lambda ❌ ❌ PROJECT_NAME fastapi-lambda ❌ ❌ LAMBDA_FUNCTION_NAME fastapi-lambda-fn ❌ ❌ Step 5: Setup GitLab CI/CD \u0026amp; Push # Copy GitLab CI file (from repository root) cd .. cp Backend-FastAPI-Docker_Build-Pipeline/.gitlab-ci.yml.example .gitlab-ci.yml # Commit and push git add .gitlab-ci.yml git commit -m \u0026#34;Add GitLab CI/CD pipeline\u0026#34; git push origin main Pipeline automatically runs:\nlint_and_scan - Semgrep security scan build_and_push - Docker build + Trivy scan + ECR push terraform_deploy - Deploy infrastructure Architecture Overview Detailed Sections The workshop is divided into sections:\nPrerequisites - Setup AWS CLI, GitLab account Setup GitLab CI/CD - Configure pipeline and variables Security Scanning - Semgrep \u0026amp; Trivy Deploy Backend - Infrastructure automatically deployed Tech Stack Component Technology Backend FastAPI (Python) Runtime AWS Lambda (Container) Database DynamoDB CI/CD GitLab CI/CD Security Semgrep (SAST), Trivy (Container Scan) Infrastructure Terraform Monitoring CloudWatch, SNS Verification After the pipeline finishes running:\n# Get API URL from GitLab job logs or: cd Backend-FastAPI-Docker_Build-Pipeline/infra terraform output api_url # Test API curl https://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com/health # Expected: {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;} Cost Estimate Lambda: ~$1-3/month (250K requests) DynamoDB: ~$5-10/month (on-demand) API Gateway: ~$1-3/month ECR: ~$0.50-1.50/month Total: ~$10-20/month (low traffic) Next Steps Read Prerequisites to setup the environment Follow Setup GitLab CI/CD to configure the pipeline View Security Scanning to understand how scanning works Check Deploy Backend to see the infrastructure being created This workshop is based on a real project folder Backend-FastAPI-Docker_Build-Pipeline. All code and configuration are ready, you just need to configure and run!\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Setup GitLab CI/CD Pipeline","tags":[],"description":"","content":"Setup GitLab CI/CD Pipeline This section guides you through setting up the GitLab CI/CD pipeline. The pipeline automatically builds, scans, and deploys your FastAPI application using Terraform.\nStep 1: Clone the Repository Clone the project repository:\ngit clone https://gitlab.com/m.quang/devsecops-aws-ver2.git cd devsecops-aws-ver2/Backend-FastAPI-Docker_Build-Pipeline Repository: https://gitlab.com/m.quang/devsecops-aws-ver2\nStep 2: Configure AWS CLI Configure AWS CLI with your credentials:\naws configure Enter the following when prompted:\nAWS Access Key ID: Your IAM user access key AWS Secret Access Key: Your IAM user secret key Default region name: ap-southeast-1 (or your preferred region) Default output format: json Verify the configuration:\naws sts get-caller-identity You should see your AWS account ID and user ARN.\nStep 3: Create JWT Secret in AWS Secrets Manager Create a JWT secret for authentication:\n# Generate a random secret (or use your own) JWT_SECRET=$(openssl rand -hex 32) # Create secret in Secrets Manager aws secretsmanager create-secret \\ --name fastapi-jwt-secret \\ --secret-string \u0026#34;$JWT_SECRET\u0026#34; \\ --region ap-southeast-1 # Get the secret ARN (save this for GitLab variables) aws secretsmanager describe-secret \\ --secret-id fastapi-jwt-secret \\ --region ap-southeast-1 \\ --query \u0026#39;ARN\u0026#39; \\ --output text Note the Secret ARN - you\u0026rsquo;ll need it for GitLab variables (e.g., arn:aws:secretsmanager:ap-southeast-1:123456789012:secret:fastapi-jwt-secret-xxxxx)\nStep 4: Configure GitLab CI/CD Variables Navigate to your GitLab project and configure CI/CD variables:\nGo to your GitLab project: https://gitlab.com/YOUR_USERNAME/devsecops-aws-ver2 Navigate to Settings → CI/CD → Variables Click Expand on \u0026ldquo;Variables\u0026rdquo; section Add the following variables: Variable Key Value Protected Masked Description AWS_ACCESS_KEY_ID Your AWS Access Key ✅ ❌ AWS access key for CI/CD AWS_SECRET_ACCESS_KEY Your AWS Secret Key ✅ ✅ AWS secret key (masked) AWS_DEFAULT_REGION ap-southeast-1 ❌ ❌ AWS region JWT_SECRET_ARN arn:aws:secretsmanager:... ❌ ❌ Secrets Manager ARN for JWT ECR_REPO_NAME fastapi-lambda ❌ ❌ ECR repository name LAMBDA_FUNCTION_NAME fastapi-lambda-fn ❌ ❌ Lambda function name PROJECT_NAME fastapi-lambda ❌ ❌ Project name for resources Important:\nMark AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY as Protected (only available in protected branches) Mark AWS_SECRET_ACCESS_KEY as Masked (hidden in logs) Use Expand button to add each variable Step 5: Create GitLab CI/CD Pipeline File The project uses GitLab CI/CD with three stages. Create or verify .gitlab-ci.yml in the root of your repository:\nstages: - lint - build - deploy variables: IMAGE_NAME: \u0026#34;fastapi-lambda\u0026#34; IMAGE_TAG: \u0026#34;${CI_COMMIT_SHORT_SHA}\u0026#34; lint_and_scan: stage: lint image: docker:24.0.7 services: - docker:24.0.7-dind before_script: - apk add --no-cache python3 py3-pip bash curl - pip3 install semgrep script: - echo \u0026#34;Running Semgrep SAST scan...\u0026#34; - semgrep --config Backend-FastAPI-Docker_Build-Pipeline/backend/semgrep.yml Backend-FastAPI-Docker_Build-Pipeline/backend/ artifacts: reports: codequality: semgrep-report.json paths: - semgrep-report.json expire_in: 1 week only: - main - merge_requests build_and_push: stage: build image: docker:24.0.7 services: - docker:24.0.7-dind before_script: - apk add --no-cache python3 py3-pip bash curl jq aws-cli - curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin - aws --version - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $ECR_URI script: - echo \u0026#34;Building Docker image...\u0026#34; - cd Backend-FastAPI-Docker_Build-Pipeline - docker build -t $IMAGE_NAME:$IMAGE_TAG backend - echo \u0026#34;Scanning filesystem with Trivy...\u0026#34; - trivy fs --exit-code 1 --severity HIGH,CRITICAL --ignorefile backend/trivyignore.txt . - echo \u0026#34;Scanning image with Trivy...\u0026#34; - trivy image --exit-code 1 --severity HIGH,CRITICAL $IMAGE_NAME:$IMAGE_TAG - echo \u0026#34;Tagging and pushing to ECR...\u0026#34; - docker tag $IMAGE_NAME:$IMAGE_TAG $ECR_URI:$IMAGE_TAG - docker push $ECR_URI:$IMAGE_TAG - echo \u0026#34;$ECR_URI:$IMAGE_TAG\u0026#34; \u0026gt; image-uri.txt artifacts: paths: - Backend-FastAPI-Docker_Build-Pipeline/image-uri.txt expire_in: 1 week only: - main terraform_deploy: stage: deploy image: hashicorp/terraform:1.9.5 before_script: - apk add --no-cache aws-cli - aws --version - cd Backend-FastAPI-Docker_Build-Pipeline script: - echo \u0026#34;Initializing Terraform...\u0026#34; - cd infra - terraform init -upgrade - echo \u0026#34;Planning Terraform changes...\u0026#34; - terraform plan -input=false -out=tfplan \\ -var \u0026#34;project_name=$PROJECT_NAME\u0026#34; \\ -var \u0026#34;region=$AWS_DEFAULT_REGION\u0026#34; \\ -var \u0026#34;jwt_secret_arn=$JWT_SECRET_ARN\u0026#34; \\ -var \u0026#34;image_uri=$ECR_URI:$IMAGE_TAG\u0026#34; - echo \u0026#34;Applying Terraform changes...\u0026#34; - terraform apply -auto-approve tfplan - echo \u0026#34;Deployment completed successfully!\u0026#34; dependencies: - build_and_push only: - main Note: Replace $ECR_URI with your actual ECR repository URI. You\u0026rsquo;ll get this after creating the ECR repository.\nStep 6: Create ECR Repository Create the ECR repository for storing Docker images:\n# Create ECR repository aws ecr create-repository \\ --repository-name fastapi-lambda \\ --image-scanning-configuration scanOnPush=true \\ --encryption-configuration encryptionType=AES256 \\ --region ap-southeast-1 # Get ECR repository URI ECR_URI=$(aws ecr describe-repositories \\ --repository-names fastapi-lambda \\ --region ap-southeast-1 \\ --query \u0026#39;repositories[0].repositoryUri\u0026#39; \\ --output text) echo \u0026#34;ECR URI: $ECR_URI\u0026#34; Add ECR_URI to GitLab CI/CD variables:\nVariable Key: ECR_URI Value: 123456789012.dkr.ecr.ap-southeast-1.amazonaws.com/fastapi-lambda (your actual URI) Step 7: Push Code and Trigger Pipeline Commit and push your code to trigger the pipeline:\n# Add .gitlab-ci.yml if you created it git add .gitlab-ci.yml git commit -m \u0026#34;Add GitLab CI/CD pipeline\u0026#34; git push origin main The pipeline will automatically:\nLint Stage: Run Semgrep SAST scan Build Stage: Build Docker image, scan with Trivy, push to ECR Deploy Stage: Run Terraform to deploy infrastructure Step 8: Monitor Pipeline Execution Go to your GitLab project Navigate to CI/CD → Pipelines Click on the running pipeline to view job logs Monitor each stage: lint_and_scan: Semgrep security scan build_and_push: Docker build and Trivy scan terraform_deploy: Infrastructure deployment After the pipeline completes successfully, you should see all stages with green checkmarks:\nStep 9: Get API Gateway URL After successful deployment, get the API Gateway URL:\ncd Backend-FastAPI-Docker_Build-Pipeline/infra terraform output api_url Or from GitLab CI/CD job logs, look for Terraform output.\nTroubleshooting Pipeline Fails at Lint Stage Check Semgrep findings in job logs Fix security issues in code Update backend/semgrep.yml if needed Pipeline Fails at Build Stage Verify ECR_URI variable is correct Check AWS credentials have ECR permissions Verify Docker image builds locally: docker build -t test backend Pipeline Fails at Deploy Stage Check Terraform logs in job output Verify all GitLab variables are set correctly Ensure IAM user has permissions for Lambda, API Gateway, DynamoDB AWS Credentials Issues Verify AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are correct Check IAM user has required permissions Test credentials: aws sts get-caller-identity The pipeline automatically triggers on every push to the main branch. You can also manually trigger it from GitLab CI/CD → Pipelines → Run Pipeline.\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: “AI-Driven Development Workshop – Shaping the Future of Software Development” Event Overview Venue: AWS Event Hall, 26th Floor – Bitexco Tower, Ho Chi Minh City\nTime: 14:00 – 16:30, Friday, October 3, 2025\nSpeakers: Mr. Toan Huynh, Ms. My Nguyen\nCoordinators: Diem My, Dai Truong, Dinh Nguyen\nObjectives Introduce AI-Driven Development (AI-DD) and the AI-Driven Development Lifecycle (AI-DLC) framework. Demonstrate practical use of Amazon Q Developer and Kiro IDE Extension. Explore how AI improves speed, productivity, quality, and innovation in software development. Discuss the evolving relationship between AI and human developers in modern software projects. Workshop Highlights 1. Opening Session – “Shaping the Future of Development” Mr. Toan Huynh opened the workshop emphasizing the paradigm shift from traditional to AI-Orchestrated Development.\nIn this model, AI serves as a co-pilot—an orchestrator that assists in planning, design, programming, testing, and deployment, while developers maintain creative control and accountability.\n2. Challenges in Traditional Development Speakers pointed out limitations in current AI-assisted tools: lack of reliability, transparency, and contextual understanding.\nThe AI-Driven Development approach resolves these issues by ensuring that AI acts as an enabler, not a decision-maker—augmenting rather than replacing human expertise.\n3. The AI-Driven Development Lifecycle (AI-DLC) AI-DLC represents an evolution of development maturity across three stages:\nLevel Description AI-Assisted Development AI supports coding tasks such as code completion, syntax checking, and documentation. AI-Driven Development AI collaborates in system design, project planning, and decision support. AI-Managed Development AI orchestrates processes automatically with human oversight for validation. Within AI-DLC, AI functions as an intelligent coordinator, ensuring consistency while developers retain final decision-making authority.\n4. Benefits of AI Integration The session “AI in Development – Outcomes” highlighted seven major benefits:\nPredictability: Better project tracking and schedule accuracy. Velocity: Faster time-to-market for new features. Quality: Reduced bugs and improved stability. Innovation: Enabling creative and experimental approaches. Engagement: Enhancing developer motivation and efficiency. Customer Satisfaction: Improved user experiences. Productivity: Shorter development cycles and increased output. 5. AI in the SDLC Speakers analyzed how AI supports each SDLC phase:\nExplore \u0026amp; Plan → Create → Test \u0026amp; Secure → Review \u0026amp; Deploy → Maintain \u0026amp; Modernize.\nAI significantly shortens time in testing, deployment, and maintenance by automating repetitive workflows and analyzing logs intelligently.\n6. Standard AI-DLC Workflow A standard AI-DLC flow consists of four key steps:\nRequirement: Product Owner collects and analyzes user needs. Design: Architects define structure, APIs, and processes. Implementation: Engineers develop and integrate code. Deployment: System is launched and monitored. AI tools assist in every step—ensuring output consistency and better communication between roles.\n7. Key Process Features The “Key Workflow Features” slide described four main characteristics:\nRole Separation: Clear distinction between business, design, and implementation tasks. AI-Enhanced Contexts: Each role uses AI differently (PM, Architect, Developer). Iterative Feedback Loops: Continuous improvement between stages. Template-Driven Approach: Standardized documentation for maintainability. 8. AI in Practice – Live Demonstrations Amazon Q Developer Integrated into IDEs such as VS Code and AWS Cloud9. Generates, tests, and documents code automatically. Assists in updating prompt.md, managing CI/CD pipelines, and suggesting AWS architectures. Demo showcased how AI creates project plans, user stories, and AWS-based architecture diagrams. Kiro IDE (Presented by Ms. My Nguyen) A powerful extension for creating structured documents: requirements.md, design.md, tasks.md. AI features include generating feature specs, defining API flows, and backend code creation. Demonstration: Building a Chat Application with authentication, including login, registration, and token handling. Key Insights and Takeaways AI should be viewed as a smart collaborator, augmenting human work across all development phases. The AI-DLC model standardizes workflow and enhances cross-team transparency. Amazon Q Developer improves productivity by automating documentation and testing. Kiro IDE demonstrates how AI can accelerate backend generation and design tasks. Integrating AI into DevSecOps is becoming a necessity for productivity and security alignment. Future developers will act as AI orchestrators—guiding, validating, and ethically deploying AI-driven processes. Practical Applications Deploy Amazon Q Developer to automate testing, code documentation, and CI/CD integration. Utilize Kiro IDE to standardize specifications and reduce manual workload. Conduct AI-Driven Sprints to measure productivity gains. Implement AI-Assisted Code Review to enhance software quality and maintain compliance. “The workshop reaffirmed that AI is not a replacement but a catalyst for innovation—enabling developers to build smarter, faster, and more reliable systems.”\nSome event photos Personal Reflection:\nThe event provided deep and practical insight into how AI redefines software engineering.\nMr. Toan Huynh’s strategic perspective and Ms. My Nguyen’s technical demonstration together highlighted that the future of software lies in collaboration between human creativity and AI precision.\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives (per CloudJourney roadmap) Deepen storage knowledge with EFS, FSx, and migration tools (Module 3). Introduce compute services: EC2 advanced features, Auto Scaling Groups, and initial container concepts (Module 4). Integrate compute with existing VPC and storage setups. Update Hugo site with Week 2 logs and organize lab artifacts. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 3 — Storage advanced: Study EFS shared file systems, FSx for Windows/Linux, configure EFS for EC2 access. 15/09/2025 15/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 3 Tuesday Module 3 — Labs: Set up EFS mount targets, FSx file shares, perform data migration using AWS MGN for storage workloads. 16/09/2025 16/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 3 Labs Wednesday Module 4 — Compute intro: Review EC2 instance types, launch templates, create Auto Scaling Groups integrated with VPC. 17/09/2025 17/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 4 Thursday Module 4 — Labs: Deploy EC2 ASG with load balancer, simulate scaling events, attach EBS volumes dynamically. 18/09/2025 18/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 4 Labs Friday Backup and restore practice: Test EFS backups with AWS Backup, review cost implications of storage/compute setups. 19/09/2025 19/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 3/4 Results \u0026amp; Achievements (Week 2) Advanced storage configured: EFS mounted on multiple EC2 instances for shared access, FSx set up for managed file systems, MGN used for initial storage migration simulation. Compute foundation built: ASG deployed with ELB for high availability, scaling policies tested under load, integrated with existing VPC and EBS/EFS. Backup strategies implemented: Automated backups for EFS and EC2 volumes via AWS Backup, restore procedures verified. Resource organization improved: Created a team guide on cleanup and cost optimization, shared via Hugo site. Week 2 logs documented, screenshots of ASG scaling and EFS mounts added to repository. Issues Encountered \u0026amp; Mitigations EFS lab encountered mount permission errors → adjusted security groups and IAM roles for NFS access. ASG scaling failed initially due to IAM policy gaps → added necessary EC2 and AutoScaling permissions. Minor cost spikes from untagged resources → implemented tagging policies and reviewed with Cost Explorer. Next Steps (Week 3) Advance Module 4: Dive into ECS, EKS, and Lambda for containerized and serverless compute. Begin databases: RDS setup and integration with compute services. Experiment with basic monitoring using CloudWatch. Prepare integration demo of storage-compute-database stack. "},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Deploy Infrastructure","tags":[],"description":"","content":"Deploy Infrastructure The infrastructure is automatically deployed by the GitLab CI/CD pipeline. The terraform_deploy job runs Terraform to create all AWS resources.\nAutomatic Deployment via Pipeline When you push code to the main branch, the pipeline automatically:\nLint Stage: Runs Semgrep security scan Build Stage: Builds Docker image, scans with Trivy, pushes to ECR Deploy Stage: Runs Terraform to deploy infrastructure The terraform_deploy job uses the GitLab CI/CD variables you configured earlier.\nInfrastructure Created The Terraform deployment creates:\nDynamoDB Tables:\nproducts - Product catalog orders - Order information users - User accounts and authentication Lambda Function:\nContainer-based Lambda using image from ECR Environment variables from Secrets Manager IAM role with least privilege permissions API Gateway HTTP API:\nRoutes all requests to Lambda function Automatic deployment on changes IAM Roles:\nLambda execution role Permissions for DynamoDB, Secrets Manager, CloudWatch CloudWatch Resources:\nLog groups for Lambda Metric alarms for errors SNS topic for alerts Terraform Module Structure The infrastructure is organized into reusable modules:\ninfra/ ├── main.tf # Orchestrates all modules ├── variables.tf # Input variables ├── outputs.tf # Output values ├── providers.tf # Provider configuration └── modules/ ├── dynamodb/ # DynamoDB tables ├── iam/ # IAM roles and policies ├── lambda_container/ # Lambda function ├── apigw/ # API Gateway HTTP API └── observability/ # CloudWatch and SNS DynamoDB Tables The DynamoDB module creates three tables:\n1. Products Table resource \u0026#34;aws_dynamodb_table\u0026#34; \u0026#34;products\u0026#34; { name = var.products_table_name billing_mode = \u0026#34;PAY_PER_REQUEST\u0026#34; hash_key = \u0026#34;product_id\u0026#34; attribute { name = \u0026#34;product_id\u0026#34; type = \u0026#34;S\u0026#34; } attribute { name = \u0026#34;category\u0026#34; type = \u0026#34;S\u0026#34; } global_secondary_index { name = \u0026#34;category-index\u0026#34; hash_key = \u0026#34;category\u0026#34; projection_type = \u0026#34;ALL\u0026#34; } } Attributes:\nproduct_id (String) - Primary key name (String) - Product name price (Number) - Product price stock (Number) - Stock quantity category (String) - Product category updated_at (String) - Last update timestamp Global Secondary Index:\ncategory-index - Query products by category 2. Orders Table resource \u0026#34;aws_dynamodb_table\u0026#34; \u0026#34;orders\u0026#34; { name = var.orders_table_name billing_mode = \u0026#34;PAY_PER_REQUEST\u0026#34; hash_key = \u0026#34;order_id\u0026#34; attribute { name = \u0026#34;order_id\u0026#34; type = \u0026#34;S\u0026#34; } attribute { name = \u0026#34;user_id\u0026#34; type = \u0026#34;S\u0026#34; } global_secondary_index { name = \u0026#34;user-index\u0026#34; hash_key = \u0026#34;user_id\u0026#34; projection_type = \u0026#34;ALL\u0026#34; } } Attributes:\norder_id (String) - Primary key user_id (String) - User who placed the order items (List) - Order items total_amount (Number) - Total order amount status (String) - Order status created_at (String) - Order creation timestamp Global Secondary Index:\nuser-index - Query orders by user 3. Users Table resource \u0026#34;aws_dynamodb_table\u0026#34; \u0026#34;users\u0026#34; { name = var.users_table_name billing_mode = \u0026#34;PAY_PER_REQUEST\u0026#34; hash_key = \u0026#34;user_id\u0026#34; attribute { name = \u0026#34;user_id\u0026#34; type = \u0026#34;S\u0026#34; } attribute { name = \u0026#34;email\u0026#34; type = \u0026#34;S\u0026#34; } global_secondary_index { name = \u0026#34;email-index\u0026#34; hash_key = \u0026#34;email\u0026#34; projection_type = \u0026#34;ALL\u0026#34; } } Attributes:\nuser_id (String) - Primary key email (String) - User email (unique) password_hash (String) - Hashed password role (String) - User role (admin/user) created_at (String) - Account creation timestamp Global Secondary Index:\nemail-index - Query users by email Lambda Function The Lambda module creates a container-based function:\nresource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;this\u0026#34; { function_name = \u0026#34;${var.project_name}-fn\u0026#34; package_type = \u0026#34;Image\u0026#34; image_uri = var.image_uri role = var.lambda_role_arn timeout = 15 memory_size = 512 environment { variables = var.environment } } Configuration:\nPackage Type: Container Image Image URI: From ECR (set via GitLab variable ECR_URI) Timeout: 15 seconds Memory: 512 MB Environment Variables: AWS_REGION PRODUCTS_TABLE ORDERS_TABLE JWT_SECRET (fetched from Secrets Manager) API Gateway HTTP API The API Gateway module creates an HTTP API:\nresource \u0026#34;aws_apigatewayv2_api\u0026#34; \u0026#34;http\u0026#34; { name = \u0026#34;${var.project_name}-http\u0026#34; protocol_type = \u0026#34;HTTP\u0026#34; } resource \u0026#34;aws_apigatewayv2_integration\u0026#34; \u0026#34;lambda\u0026#34; { api_id = aws_apigatewayv2_api.http.id integration_type = \u0026#34;AWS_PROXY\u0026#34; integration_uri = var.lambda_arn } resource \u0026#34;aws_apigatewayv2_route\u0026#34; \u0026#34;any\u0026#34; { api_id = aws_apigatewayv2_api.http.id route_key = \u0026#34;$default\u0026#34; target = \u0026#34;integrations/${aws_apigatewayv2_integration.lambda.id}\u0026#34; } Features:\nHTTP API (v2) - Lower latency and cost $default route - All requests go to Lambda Automatic deployment on changes IAM Roles The IAM module creates a Lambda execution role:\nresource \u0026#34;aws_iam_role\u0026#34; \u0026#34;lambda\u0026#34; { name = \u0026#34;${var.project_name}-lambda-role\u0026#34; assume_role_policy = data.aws_iam_policy_document.lambda_assume.json } data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;lambda_policy\u0026#34; { statement { actions = [\u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34;] resources = [\u0026#34;*\u0026#34;] } statement { actions = [\u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34;] resources = [ var.products_table_arn, var.orders_table_arn, var.users_table_arn, \u0026#34;${var.products_table_arn}/index/*\u0026#34;, \u0026#34;${var.orders_table_arn}/index/*\u0026#34;, \u0026#34;${var.users_table_arn}/index/*\u0026#34; ] } statement { actions = [\u0026#34;secretsmanager:GetSecretValue\u0026#34;] resources = [var.jwt_secret_arn] } } Permissions:\nCloudWatch Logs: Create log groups and streams DynamoDB: Full CRUD operations on all three tables and their indexes Secrets Manager: Read JWT secret value Monitoring and Observability The observability module creates:\nCloudWatch Log Group: /aws/lambda/${lambda_name} CloudWatch Alarm: Triggers when error count \u0026gt; 0 SNS Topic: Receives alarm notifications Get Deployment Outputs After the pipeline completes successfully, get the API Gateway URL:\nOption 1: From GitLab CI/CD Job Logs Go to CI/CD → Pipelines Click on the completed pipeline Click on terraform_deploy job Scroll to the end of logs to find Terraform outputs: Outputs: api_url = \u0026#34;https://abc123xyz.execute-api.ap-southeast-1.amazonaws.com\u0026#34; lambda_arn = \u0026#34;arn:aws:lambda:ap-southeast-1:123456789012:function:fastapi-lambda-fn\u0026#34; Option 2: From AWS Console Navigate to API Gateway → APIs Find your API (name: fastapi-lambda-http) Copy the Invoke URL Option 3: Using AWS CLI # Get API Gateway URL aws apigatewayv2 get-apis \\ --region ap-southeast-1 \\ --query \u0026#39;Items[?Name==`fastapi-lambda-http`].ApiEndpoint\u0026#39; \\ --output text Verify Deployment Test the health endpoint:\nAPI_URL=\u0026#34;https://abc123xyz.execute-api.ap-southeast-1.amazonaws.com\u0026#34; curl $API_URL/health Expected response:\n{\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;} Manual Deployment (Optional) If you need to deploy manually (not recommended):\ncd Backend-FastAPI-Docker_Build-Pipeline/infra # Initialize Terraform terraform init # Plan deployment terraform plan \\ -var \u0026#34;project_name=fastapi-lambda\u0026#34; \\ -var \u0026#34;region=ap-southeast-1\u0026#34; \\ -var \u0026#34;jwt_secret_arn=arn:aws:secretsmanager:...\u0026#34; \\ -var \u0026#34;image_uri=123456789012.dkr.ecr.ap-southeast-1.amazonaws.com/fastapi-lambda:latest\u0026#34; # Apply deployment terraform apply \\ -var \u0026#34;project_name=fastapi-lambda\u0026#34; \\ -var \u0026#34;region=ap-southeast-1\u0026#34; \\ -var \u0026#34;jwt_secret_arn=arn:aws:secretsmanager:...\u0026#34; \\ -var \u0026#34;image_uri=123456789012.dkr.ecr.ap-southeast-1.amazonaws.com/fastapi-lambda:latest\u0026#34; The pipeline automatically handles deployment. Manual deployment is only needed for troubleshooting or initial setup before the pipeline is configured.\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"DevSecOps Security Testing &amp; Configuration","tags":[],"description":"","content":"DevSecOps Security Testing \u0026amp; Configuration This section focuses on security testing and manual configuration of security scanning tools for the FastAPI Backend. You will learn how to configure and test Semgrep, Trivy, and ECR scanning manually.\nDevSecOps Testing Overview The FastAPI Backend implements defense-in-depth security through multiple testing layers:\nLayer Tool Type Purpose Code Semgrep SAST Static code analysis Dependencies Trivy FS Dependency Scan Scan filesystem for vulnerabilities Container Trivy Image Container Scan Scan Docker image Registry ECR Scan Image Scan Automatic scan on push Step 1: Manual Semgrep Configuration \u0026amp; Testing Install Semgrep # Install Semgrep pip install semgrep # Or using Docker docker pull returntocorp/semgrep Configure Custom Rules Edit Backend-FastAPI-Docker_Build-Pipeline/backend/semgrep.yml:\nrules: - id: jwt-hardcoded-secret message: Avoid hardcoding JWT secrets; use Secrets Manager/env. severity: ERROR languages: [python] pattern: | jwt.encode($P, $S, ...) metavariable-pattern: metavariable: $S pattern: \u0026#34;\u0026#39;$SECRET\u0026#39;\u0026#34; - id: fastapi-debug message: Do not run uvicorn with reload or debug in production. severity: WARNING languages: [python] pattern-either: - pattern: uvicorn.run(..., reload=True, ...) - pattern: uvicorn.run(..., debug=True, ...) Run Semgrep Scan Manually cd Backend-FastAPI-Docker_Build-Pipeline/backend # Basic scan semgrep --config semgrep.yml . # Scan with JSON output semgrep --config semgrep.yml . --json -o semgrep-results.json # Scan with HTML report semgrep --config semgrep.yml . --html -o semgrep-report.html Step 2: Manual Trivy Configuration \u0026amp; Testing Install Trivy # Install Trivy curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin # Verify installation trivy --version Run Filesystem Scan cd Backend-FastAPI-Docker_Build-Pipeline/backend # Basic filesystem scan trivy fs --severity HIGH,CRITICAL . # Scan with JSON output trivy fs --severity HIGH,CRITICAL . --format json -o trivy-fs-results.json Build and Scan Container Image # Build Docker image cd Backend-FastAPI-Docker_Build-Pipeline docker build -t fastapi-lambda:test backend/ # Scan container image trivy image --severity HIGH,CRITICAL fastapi-lambda:test # Scan with exit code (fails on findings) trivy image --exit-code 1 --severity HIGH,CRITICAL fastapi-lambda:test Example Output:\nScanning filesystem with Trivy... 2025-11-18T03:44:45Z\tINFO\tDetected OS\tfamily=\u0026#34;amazon\u0026#34; version=\u0026#34;2\u0026#34; 2025-11-18T03:45:14Z\tINFO\t[python-pkg] Detecting vulnerabilities... Report Summary ┌──────────────────────────────────────────────────────────┬────────────┬─────────────────┬─────────┐ │ Target │ Type │ Vulnerabilities │ Secrets │ ├──────────────────────────────────────────────────────────┼────────────┼─────────────────┼─────────┤ │ fastapi-lambda:test (amazon 2) │ amazon │ 0 │ - │ │ var/task/fastapi-0.115.0.dist-info/METADATA │ python-pkg │ 1 │ - │ └──────────────────────────────────────────────────────────┴────────────┴─────────────────┴─────────┘ CRITICAL: 1 (CVE-2024-XXXXX in fastapi==0.115.0) Step 3: Manual ECR Image Scanning Configuration Enable ECR Image Scanning # Enable scanning on existing repository aws ecr put-image-scanning-configuration \\ --repository-name fastapi-lambda \\ --image-scanning-configuration scanOnPush=true \\ --region ap-southeast-1 View ECR Scan Results # Get scan findings aws ecr describe-image-scan-findings \\ --repository-name fastapi-lambda \\ --image-id imageTag=test \\ --region ap-southeast-1 Security Testing Workflow Create security-test.sh script to run all security tests:\n#!/bin/bash set -e echo \u0026#34;Starting DevSecOps Security Testing...\u0026#34; # 1. Semgrep SAST Scan echo \u0026#34;Running Semgrep SAST scan...\u0026#34; cd Backend-FastAPI-Docker_Build-Pipeline/backend semgrep --config semgrep.yml . --json -o ../semgrep-results.json # 2. Trivy Filesystem Scan echo \u0026#34;Running Trivy filesystem scan...\u0026#34; trivy fs --severity HIGH,CRITICAL . --format json -o ../trivy-fs-results.json # 3. Build and Scan Container echo \u0026#34;Building Docker image...\u0026#34; docker build -t fastapi-lambda:security-test backend/ echo \u0026#34;Running Trivy container scan...\u0026#34; trivy image --severity HIGH,CRITICAL --format json -o ../trivy-image-results.json fastapi-lambda:security-test echo \u0026#34;Security testing complete!\u0026#34; View Results in GitLab CI/CD After pushing code, view security scan results:\nNavigate to CI/CD → Pipelines Click on pipeline run View lint_and_scan job logs for Semgrep results View build_and_push job logs for Trivy results Best Practices Run Before Push: Test locally before pushing Fail Fast: Use --exit-code 1 to fail builds on critical findings Severity Threshold: Only block on HIGH and CRITICAL findings Regular Updates: Update Semgrep rules and Trivy database regularly "},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/5-workshop/5.2-prerequiste/","title":"Prerequisites","tags":[],"description":"","content":"Prerequisites Before starting this workshop, ensure you have the following requirements ready.\n1. AWS Account You need an AWS account with appropriate permissions. If you don\u0026rsquo;t have one:\nGo to AWS Console Click Create an AWS Account Follow the registration process Enable MFA for the root account (recommended) This workshop will create resources that incur costs. Estimated cost is ~$5-10 if cleaned up within a few hours. Make sure to complete the cleanup section at the end.\n2. IAM User with Required Permissions Create an IAM user with programmatic access for GitLab CI/CD:\nCreate IAM User # Create IAM user aws iam create-user --user-name gitlab-ci-user # Create access key aws iam create-access-key --user-name gitlab-ci-user Save the Access Key ID and Secret Access Key - you\u0026rsquo;ll need them for GitLab CI/CD variables.\nAttach Required Policies The IAM user needs permissions for:\nLambda, API Gateway, DynamoDB ECR (Elastic Container Registry) Secrets Manager CloudWatch, SNS IAM (for creating roles) S3 (for Terraform state, if using remote backend) Option 1: Administrator Access (for workshop)\naws iam attach-user-policy \\ --user-name gitlab-ci-user \\ --policy-arn arn:aws:iam::aws:policy/AdministratorAccess Option 2: Custom Policy (recommended for production)\nCreate a custom policy with least privilege permissions for the services listed above.\n3. GitLab Account Sign up at GitLab Create a new project/repository Fork or clone the workshop repository 4. AWS CLI Configuration Install and configure AWS CLI on your local machine:\nInstall AWS CLI # Windows (PowerShell) msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi # macOS brew install awscli # Linux curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Configure AWS CLI aws configure Enter the following when prompted:\nAWS Access Key ID: Your IAM user access key AWS Secret Access Key: Your IAM user secret key Default region name: ap-southeast-1 (or your preferred region) Default output format: json Verify AWS CLI Configuration # Check AWS CLI version aws --version # Verify credentials aws sts get-caller-identity You should see your AWS account ID and user ARN.\n5. Clone Workshop Repository Clone the project repository:\ngit clone https://gitlab.com/m.quang/devsecops-aws-ver2.git cd devsecops-aws-ver2/Backend-FastAPI-Docker_Build-Pipeline Repository: https://gitlab.com/m.quang/devsecops-aws-ver2\n6. GitLab CI/CD Variables Setup Before running the pipeline, you need to configure GitLab CI/CD variables. This will be covered in detail in the next section, but here\u0026rsquo;s a quick overview:\nRequired Variables:\nAWS_ACCESS_KEY_ID - AWS access key AWS_SECRET_ACCESS_KEY - AWS secret key (masked) AWS_DEFAULT_REGION - AWS region JWT_SECRET_ARN - Secrets Manager ARN for JWT ECR_URI - ECR repository URI PROJECT_NAME - Project name LAMBDA_FUNCTION_NAME - Lambda function name 7. Verify Setup Run the following commands to verify your setup:\n# Check AWS CLI aws --version aws sts get-caller-identity # Check Git git --version # Check Docker (optional, for local testing) docker --version Workshop Resources The repository contains all the code and configurations needed for this workshop:\nThe workshop repository contains:\nBackend: FastAPI application code with layered architecture Infrastructure: Terraform modules for all AWS resources (DynamoDB, Lambda, API Gateway, IAM, Observability) Pipeline: GitLab CI/CD configuration (.gitlab-ci.yml) Security: Semgrep and Trivy scanning configurations Docker: Dockerfile for Lambda container image Next Steps After completing prerequisites:\n✅ AWS account created ✅ IAM user with access keys ✅ AWS CLI configured ✅ Repository cloned ⏭️ Configure GitLab CI/CD variables (next section) ⏭️ Set up GitLab CI/CD pipeline (next section) "},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Secure Serverless Application Proposal 1. Project Summary The project operates a serverless application on AWS with a fully automated CI/CD pipeline. When code is pushed to GitLab, the pipeline automatically scans for security vulnerabilities (Semgrep, Trivy), builds containers, provisions infrastructure via Terraform, and deploys images to Amazon ECR. Users access the application through Route 53 → CloudFront → WAF → API Gateway, authenticated by Cognito, invoking Lambda functions for business logic and storing data in DynamoDB. The system continuously monitors through CloudWatch, CloudTrail, and GuardDuty, sending alerts via EventBridge → SNS upon incidents or threats.\n2. Problem Statement Current Problems Many organizations struggle with deploying serverless applications due to error-prone manual processes, lack of security controls, and scalability challenges. Specific issues include:\nManual deployment processes lack consistent security checks, often missing vulnerability scanning and automated testing steps Existing systems do not provide high-performance content delivery (CDN) nor application-layer protection against DDoS and bot attacks Secrets, encryption keys, and IAM permissions are difficult to manage end-to-end across the CI/CD pipeline while maintaining least-privilege principles Limited observability causes slow incident response and delayed threat detection Solution The project delivers a complete serverless architecture with automated CI/CD, integrated security, and comprehensive monitoring. The deployment pipeline automates from GitLab through CodePipeline, CodeBuild, Semgrep (SAST), Trivy (container scanning), Terraform (IaC), and CodeDeploy, ensuring every change is tested and security-scanned. The delivery layer uses Route 53, CloudFront, and WAF to accelerate and protect the application. The serverless backend with Cognito (authentication), API Gateway, Lambda, DynamoDB, Secrets Manager, and KMS ensures data security. A multi-layer monitoring system (CloudWatch, CloudTrail, GuardDuty, EventBridge, SNS) detects and alerts on incidents instantly.\nBenefits and Return on Investment (ROI) The solution delivers significant technical and financial benefits: full automation reduces deployment time by 80-90% and eliminates manual errors, integrated security reduces vulnerability risks, CDN and serverless architecture ensure high performance with automatic scaling, and centralized monitoring enables 70-80% faster incident detection. Financially, operational costs are estimated at ~$20-40/month for small to medium scale, significantly lower than traditional infrastructure, with no upfront hardware investment and 60-70% reduction in operational labor costs. Expected payback period is 1-2 months, while providing an extensible foundation for future projects.\n3. System Architecture The architecture is divided into three domains:\nCI/CD Pipeline: Running on GitLab and AWS CodePipeline to build containers, scan security (Semgrep, Trivy), provision infrastructure via Terraform, and deploy images to Amazon ECR Content Delivery \u0026amp; Protection Layer: Using Route 53, AWS WAF, and CloudFront to accelerate and secure user access Serverless Application Core: Located in ap-southeast-1 region with Cognito (authentication), API Gateway, Lambda (business logic), DynamoDB (storage), KMS and Secrets Manager (security), and the observability suite (CloudWatch, CloudTrail, GuardDuty, EventBridge, SNS) AWS Services Used\nGitLab Actions, AWS CodePipeline, CodeBuild, CodeDeploy Semgrep, Trivy Terraform, Amazon ECR Amazon Cognito, Amazon API Gateway, AWS Lambda, Amazon DynamoDB AWS WAF, Amazon CloudFront, Amazon Route 53 AWS Secrets Manager, AWS Key Management Service (KMS) Amazon CloudWatch, AWS CloudTrail, Amazon GuardDuty, Amazon EventBridge, Amazon SNS Component Design\nCI/CD: A Git push triggers the pipeline to run Semgrep (SAST), build containers in CodeBuild, scan with Trivy, execute Terraform Plan/Apply, and deliver via CodeDeploy. Delivery \u0026amp; Protection: Route 53 handles DNS → CloudFront caches content → WAF filters malicious traffic before reaching the APIs. Application Services: Cognito issues tokens, API Gateway validates requests and forwards them to Lambda, which processes business logic and reads/writes DynamoDB. Secrets \u0026amp; Encryption: Secrets Manager stores sensitive information, KMS encrypts data and keys. Monitoring \u0026amp; Alerting: CloudWatch aggregates metrics/logs, CloudTrail records audit trails, GuardDuty detects threats, EventBridge routes anomalies to SNS notifications. 4. Technical Implementation Model the infrastructure with Terraform: logical VPC boundaries, least-privilege IAM roles, API Gateway, Lambda, DynamoDB, and security controls. Configure GitLab CI/CD with AWS CodePipeline/CodeBuild/CodeDeploy using cross-account IAM roles. Enforce Semgrep and Trivy scans in every pipeline run, failing builds on high-severity findings. Define Terraform modules for Lambda (container images in ECR), API Gateway, CloudFront, WAF, and Cognito resources. Enable CloudWatch log groups, metric filters, GuardDuty, CloudTrail; create EventBridge rules that fan out to SNS alerts. Implement automated rollback via CodeDeploy deployment groups and versioned Terraform state. 5. Timeline \u0026amp; Milestones Phase Schedule Key Deliverables Kick-off Week 1 Requirements intake, detailed design, IAM role matrix IaC \u0026amp; CI/CD Setup Week 2 Terraform baseline, GitLab pipeline, CodePipeline integration Security Integration Week 3 Semgrep, Trivy, WAF rules, Cognito, Secrets Manager Backend Completion Week 4 Lambda handlers, API Gateway routes, DynamoDB schema, unit tests Deployment \u0026amp; Testing Week 5 End-to-end pipeline run, integration and CDN performance testing Operations \u0026amp; Handover Week 6 Optimize monitoring systems, configure intelligent alerting, finalize documentation Post-Deployment Week 7+ Continuous performance and cost monitoring, configuration optimization, feature expansion as needed, periodic security updates 6. Budget Estimation AWS Service Primary Billing Factor Estimated Cost (USD) AWS Lambda \u0026amp; API Gateway 250,000 requests/month ~$1.00 - $3.00 Amazon DynamoDB 5 GB storage, 5 RCU/5 WCU (Provisioned) ~$5.00 - $10.00 CI/CD (CodePipeline/CodeBuild) 5 deployments (250 build minutes) ~$2.25 - $4.00 Amazon ECR 5 GB image storage ($0.10/GB) ~$0.50 - $1.50 Route 53 1 Hosted Zone + queries ~$0.54 CloudFront / WAF 15 GB Data Transfer Out, basic WAF usage ~$9.50 - $15.50 Security \u0026amp; Monitoring GuardDuty, KMS, Secrets Manager, minimal logs ~$2.00 - $5.00 Total Cost ~$20.79 - $39.54 USD/month Note: Actual spend varies with traffic and configuration; costs can drop further by shutting down dev/test environments. No hardware expenditure required.\n7. Risk Assessment Risk Impact Likelihood Mitigation Undetected vulnerabilities High Medium Mandatory Semgrep/Trivy scans, enable AWS Inspector, periodic manual reviews Misconfigured Terraform causing downtime High Low Peer review Terraform plans, use staging environments, back up state Unexpected CDN or Terraform cost spikes Medium Medium Configure AWS Budgets, tune CloudFront cache policies, monitor spend Cognito authentication issues Medium Low Implement automated auth tests, maintain backup user pool in another Region Mitigation Strategies Security: Integrate automated security scanning (Semgrep, Trivy) into every pipeline, enable AWS Inspector periodically, conduct code reviews and quarterly security audits Infrastructure Configuration: Apply Infrastructure as Code (Terraform) with version control, peer review for all changes, use staging environments for testing before production, regularly backup Terraform state Cost Management: Set up AWS Budgets with 80% and 100% alert thresholds, configure optimized CloudFront cache policies, monitor costs daily via Cost Explorer, automatically shut down dev/test resources when unused Authentication \u0026amp; Access: Deploy automated testing for API authentication, maintain backup Cognito user pool in another region, enable multi-factor authentication (MFA) for administrative accounts Contingency Plan Security Incidents: Upon detection of critical vulnerabilities, immediately rollback to previous version via CodeDeploy, isolate affected resources, notify security team and proceed with patching Downtime from Misconfiguration: Use Terraform state backup to restore previous configuration, temporarily route traffic to staging environment if needed, maintain runbook for quick rollback Budget Overrun: Automatically shut down non-essential services when alert threshold is reached, optimize CloudFront cache hit ratio, consider switching to Reserved Capacity for DynamoDB if long-term usage Authentication Failures: Switch to backup Cognito user pool in another region, utilize API Gateway caching to reduce load, maintain fallback authentication mechanism 8. Expected Outcomes A secure, fully automated CI/CD pipeline where every change is scanned and tested before reaching production. A low-latency serverless backend served through CDN and API Gateway with pervasive data encryption. Centralized observability that surfaces incidents instantly and supports high availability SLAs. Optimized access control and cost efficiency delivered through least-privilege IAM and serverless scaling. An extensible infrastructure foundation ready for future features or additional AWS Regions. "},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"AI/ML/GenAI on AWS Workshop Event Information Date \u0026amp; Time: Saturday, November 15, 2025, 8:30 AM – 12:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent Purpose The workshop was designed to provide hands-on experience with AWS AI/ML services, focusing on Amazon SageMaker for traditional machine learning workflows and Amazon Bedrock for generative AI applications. The event aimed to help participants understand the practical implementation of AI/ML solutions on AWS and explore the latest capabilities in generative AI.\nAgenda Overview 8:30 – 9:00 AM | Welcome \u0026amp; Introduction Participant registration and networking opportunities Workshop overview and learning objectives presentation Ice-breaker activity to foster collaboration Overview of the AI/ML landscape in Vietnam 9:00 – 10:30 AM | AWS AI/ML Services Overview Amazon SageMaker – End-to-End ML Platform\nData Preparation and Labeling: Understanding how to prepare datasets for machine learning, including data cleaning, feature engineering, and automated labeling capabilities Model Training, Tuning, and Deployment: Exploring SageMaker\u0026rsquo;s training infrastructure, hyperparameter tuning, and model deployment options including real-time and batch inference Integrated MLOps Capabilities: Learning about SageMaker\u0026rsquo;s built-in MLOps features for model versioning, monitoring, and automated retraining pipelines Live Demo: SageMaker Studio Walkthrough\nThe demonstration showcased the unified development environment for machine learning, including:\nJupyter notebook integration for interactive development Experiment tracking and model registry Visual workflow builder for MLOps pipelines Integration with other AWS services for data processing 10:30 – 10:45 AM | Coffee Break Networking session with refreshments and informal discussions about AI/ML use cases.\n10:45 AM – 12:00 PM | Generative AI with Amazon Bedrock Foundation Models: Claude, Llama, Titan – Comparison \u0026amp; Selection Guide\nUnderstanding different foundation models available on Bedrock Comparison of model capabilities, use cases, and performance characteristics Best practices for selecting the right model for specific business needs Cost considerations and optimization strategies Prompt Engineering: Techniques, Chain-of-Thought Reasoning, Few-shot Learning\nPrompt Engineering Fundamentals: Learning how to craft effective prompts to get desired outputs from language models Chain-of-Thought Reasoning: Understanding how to guide models through step-by-step reasoning processes for complex problem-solving Few-shot Learning: Techniques for providing examples to improve model performance on specific tasks without fine-tuning Retrieval-Augmented Generation (RAG): Architecture \u0026amp; Knowledge Base Integration\nRAG Architecture Overview: Understanding how RAG combines retrieval of relevant information with generative capabilities Knowledge Base Integration: Learning to connect Bedrock with vector databases and knowledge bases (Amazon OpenSearch, Amazon Kendra) Implementation Patterns: Best practices for building RAG applications that provide accurate, context-aware responses Bedrock Agents: Multi-step Workflows and Tool Integrations\nAgent Architecture: Understanding how Bedrock Agents can orchestrate complex multi-step workflows Tool Integration: Learning to connect agents with external APIs, databases, and AWS services Workflow Design: Patterns for designing agent-based applications that can handle complex user requests Guardrails: Safety and Content Filtering\nContent Safety: Understanding Bedrock Guardrails for filtering harmful or inappropriate content Custom Policies: Learning to configure custom content filters based on business requirements Compliance and Governance: Best practices for ensuring AI applications meet regulatory and ethical standards Live Demo: Building a Generative AI Chatbot using Bedrock\nThe demonstration walked through creating a complete chatbot application:\nSetting up Bedrock foundation model Implementing RAG with knowledge base integration Configuring Bedrock Agents for multi-turn conversations Adding Guardrails for content safety Deploying the chatbot application Key Highlights Comprehensive ML Platform: SageMaker provides a complete end-to-end solution for machine learning, from data preparation to model deployment and monitoring Generative AI Capabilities: Amazon Bedrock offers access to multiple foundation models, making it easy to experiment and choose the right model for each use case RAG Architecture: The RAG pattern enables building AI applications that can access and utilize specific knowledge bases, improving accuracy and relevance Production-Ready MLOps: SageMaker\u0026rsquo;s integrated MLOps capabilities simplify the process of deploying and maintaining ML models in production Safety First: Bedrock Guardrails ensure that generative AI applications are safe, compliant, and aligned with business values Key Learnings SageMaker Studio provides a unified interface for the entire ML lifecycle, significantly improving developer productivity Foundation model selection is crucial and depends on specific use cases, performance requirements, and cost constraints Prompt engineering is a critical skill that can dramatically improve model outputs without requiring fine-tuning RAG architecture is essential for building AI applications that need access to specific, up-to-date information Bedrock Agents enable building sophisticated AI applications that can handle complex, multi-step workflows Content safety must be considered from the beginning when building generative AI applications Application to My Work Experiment with SageMaker: Set up SageMaker Studio to explore ML model development for data analysis projects Build RAG Applications: Implement RAG architecture using Bedrock and knowledge bases for internal documentation and Q\u0026amp;A systems Prompt Engineering Practice: Develop prompt engineering skills by creating templates and best practices for common use cases MLOps Integration: Apply SageMaker\u0026rsquo;s MLOps capabilities to automate model training and deployment pipelines Safety Implementation: Integrate Bedrock Guardrails into any generative AI applications to ensure content safety Personal Experience This workshop provided an excellent hands-on introduction to AWS AI/ML services:\nThe SageMaker Studio demo was particularly impressive, showing how a unified platform can streamline the entire ML workflow Learning about RAG architecture was eye-opening, as it demonstrated how to build AI applications that can leverage specific knowledge bases The Bedrock Agents demonstration showed the potential for building sophisticated AI applications that can handle complex workflows The practical focus on prompt engineering provided immediately applicable skills for working with language models Understanding Guardrails helped me appreciate the importance of safety and compliance in AI applications Takeaways Start with Use Cases: Always begin by identifying specific business problems before selecting AI/ML solutions Foundation Models are Powerful: Pre-trained foundation models can solve many problems without custom training RAG is Essential: For applications requiring specific knowledge, RAG architecture is the way to go MLOps Matters: Proper MLOps practices are crucial for maintaining ML models in production Safety Cannot be Overlooked: Content filtering and safety measures must be integrated from the start Continuous Learning: The AI/ML landscape evolves rapidly, requiring continuous learning and experimentation Event Photos "},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Sharing a vision of a more connected world with AWS IoT AWS IoT VP Yasser Alsaied outlines a strategic shift from horizontal platforms to vertical industry-led solutions designed to deliver specific business outcomes for sectors like automotive and manufacturing. The company commits to continued investment in specialized services (such as AWS IoT SiteWise and FleetWise) and partner networks to simplify deployment from the edge to the cloud. Amazon serves as a major internal customer, validating these technologies at scale through their use in logistics robotics and \u0026ldquo;Just Walk Out\u0026rdquo; retail systems. Future trends focus on IoT becoming a standard business expectation, driving sustainability initiatives and operational efficiency through simplified tools. This approach aims to reduce costs and technical barriers, allowing customers to focus on value rather than infrastructure integration.\nBlog 2 - New and updated courses from AWS Training and Certification in March 2023 In March 2023, AWS Training and Certification released 27 new digital products, featuring 14 AWS Builder Labs and specialized courses for partners and executives. The AWS Skill Builder platform added free courses covering technical topics such as Robotics, IoT, .NET workloads, and data migration, along with strategic content tailored for CFOs and CIOs. In addition to expanding the Team subscription to 50 countries, AWS announced the temporary removal of exam labs from the AWS Certified SysOps Administrator – Associate exam to improve the assessment experience.\nBlog 3 - Hannover Messe: Discover an end-to-end industrial data strategy with AWS At Hannover Messe 2023, AWS highlighted a comprehensive industrial data strategy through the Industrial Data Fabric (IDF) architecture, designed to help manufacturers unify disconnected data sources and scale digital transformation from proof-of-concept to production. In collaboration with strategic partners like Bosch, Siemens, and MHP, AWS demonstrated these capabilities through interactive demos covering the automotive manufacturing value chain, smart supply chains, and carbon footprint tracking. Beyond the exhibits, attendees had access to over 50 deep-dive presentations on industrial automation and the opportunity to book 1:1 consultations with AWS experts to address specific business challenges.\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives (per CloudJourney roadmap) Explore advanced compute: Containers with ECS/EKS and serverless with Lambda (Module 4). Introduce relational databases: RDS setup, multi-AZ, backups (Databases in Module 1 Explore). Integrate services with prior networking and storage. Enhance monitoring with CloudWatch basics. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 4 — Containers intro: Study ECS task definitions, Fargate launch types; deploy simple container app. 22/09/2025 22/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 4 Tuesday Module 4 — Labs: Create ECS cluster, run tasks with ECR images, integrate with ALB. 23/09/2025 23/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 4 Labs Wednesday Module 4 — Serverless: Lambda functions, triggers from S3/EC2, API Gateway integration. 24/09/2025 24/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 4 Thursday Databases — RDS: Provision RDS instance (MySQL/PostgreSQL), connect from EC2, enable multi-AZ. 25/09/2025 25/09/2025 https://cloudjourney.awsstudygroup.com/ - Explore AWS Services Friday Monitoring basics: Set up CloudWatch alarms for EC2 CPU, RDS storage; log Lambda invocations. 26/09/2025 26/09/2025 https://cloudjourney.awsstudygroup.com/ - Monitoring Results \u0026amp; Achievements (Week 3) Containerized app deployed: ECS Fargate cluster running web app, scaled with ALB, images pushed to ECR. Serverless workflow established: Lambda triggered by S3 uploads, integrated with API Gateway for REST endpoints. RDS operational: Multi-AZ MySQL instance created, connected via EC2 app, automated snapshots configured. Monitoring implemented: CloudWatch dashboards for key metrics, alarms notified via SNS. Integration demo prepared: Simple app stack with EC2-Lambda-RDS-CloudWatch. Issues Encountered \u0026amp; Mitigations ECS task failed to pull image → verified ECR permissions and VPC endpoint setup. Lambda cold starts impacted performance → optimized code and increased memory allocation. RDS connection timeout → updated security groups to allow EC2 subnet traffic. Next Steps (Week 4) Deepen databases: NoSQL with DynamoDB, caching with ElastiCache. Advance monitoring: CloudTrail for auditing, X-Ray for tracing. Start migration planning from Module 2. Document integration patterns in Hugo. "},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/5-workshop/5.3-s3-vpc/","title":"Setting up GitLab CI/CD Pipeline","tags":[],"description":"","content":"Setting up GitLab CI/CD Pipeline In this section, you will set up a GitLab CI/CD pipeline to automate building, security scanning, and deploying your FastAPI application.\nPipeline Overview The GitLab CI/CD pipeline consists of three stages:\nStage Job Tool Purpose lint lint_and_scan Semgrep Static Application Security Testing (SAST) build build_and_push Docker + Trivy Build container image and scan for vulnerabilities deploy terraform_deploy Terraform Provision/update infrastructure Content Setup GitLab CI/CD Pipeline - Config AWS CLI, GitLab variables, and pipeline Security Scanning Configuration - Semgrep and Trivy integration Pipeline Flow After the pipeline completes successfully, you should see all stages with green checkmarks:\nGitLab CI/CD Configuration The pipeline is defined in .gitlab-ci.yml:\nstages: - lint - build - deploy lint_and_scan: stage: lint script: - semgrep --config backend/semgrep.yml backend/ build_and_push: stage: build script: - docker build -t fastapi-lambda:$CI_COMMIT_SHORT_SHA backend - trivy fs --severity HIGH,CRITICAL . - trivy image --severity HIGH,CRITICAL fastapi-lambda:$CI_COMMIT_SHORT_SHA - docker push $ECR_URI:$CI_COMMIT_SHORT_SHA terraform_deploy: stage: deploy script: - cd infra - terraform init - terraform apply -auto-approve Security Scanning The pipeline includes two types of security scanning:\nSemgrep (SAST): Static code analysis for security vulnerabilities Trivy: Container vulnerability scanning (filesystem + image) Key Features ✅ Automatic Trigger: Runs on every push to main branch ✅ Security First: Fails build on critical security findings ✅ Container Image: Builds and scans Docker image ✅ Infrastructure as Code: Terraform automatically deploys infrastructure ✅ GitLab Variables: All secrets stored in GitLab CI/CD variables Quick Setup Config AWS CLI (local) Create JWT Secret (AWS Secrets Manager) Create ECR Repository (AWS ECR) Config GitLab Variables (7 variables) Push Code → Pipeline runs automatically The pipeline automatically handles everything: build, scan, and deploy. You only need to configure AWS CLI and GitLab variables once.\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Testing the API","tags":[],"description":"","content":"Testing the API After the pipeline deploys the backend, you can test all API endpoints.\nStep 1: Get API Gateway URL # Option 1: From GitLab job logs (terraform_deploy job) # Look for: api_url = \u0026#34;https://...\u0026#34; # Option 2: From Terraform output cd Backend-FastAPI-Docker_Build-Pipeline/infra API_URL=$(terraform output -raw api_url) echo $API_URL Example: https://0bb7ewqnga.execute-api.ap-southeast-1.amazonaws.com\nStep 2: Test Health Endpoint curl $API_URL/health Expected response:\n{\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;} Step 3: Access Swagger UI Open in browser:\nhttps://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com/docs The Swagger UI allows you to:\nView all available endpoints Test endpoints directly from the browser See request/response schemas Try out authentication Step 4: Register a User curl -X POST $API_URL/auth/register \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;user@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;SecurePassword123!\u0026#34; }\u0026#39; Expected response:\n{\u0026#34;message\u0026#34;: \u0026#34;registered\u0026#34;} Step 5: Login and Get JWT Token curl -X POST $API_URL/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;user@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;SecurePassword123!\u0026#34; }\u0026#39; Expected response:\n{ \u0026#34;access_token\u0026#34;: \u0026#34;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;bearer\u0026#34; } Save the token:\nTOKEN=$(curl -s -X POST $API_URL/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;email\u0026#34;: \u0026#34;user@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;SecurePassword123!\u0026#34;}\u0026#39; \\ | jq -r \u0026#39;.access_token\u0026#39;) echo \u0026#34;Token: $TOKEN\u0026#34; Step 6: List Products (Public) curl $API_URL/products Expected response:\n[] Step 7: Create Product (Admin Only) Note: Regular users cannot create products. You need an admin user. For testing, you can manually create an admin user in DynamoDB or modify the registration logic.\ncurl -X POST $API_URL/products \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -d \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;Laptop\u0026#34;, \u0026#34;price\u0026#34;: 999.99, \u0026#34;stock\u0026#34;: 10, \u0026#34;category\u0026#34;: \u0026#34;electronics\u0026#34; }\u0026#39; If user is not admin, you\u0026rsquo;ll get:\n{\u0026#34;detail\u0026#34;: \u0026#34;Admin only\u0026#34;} Step 8: Get Product by ID PRODUCT_ID=\u0026#34;your-product-id\u0026#34; curl $API_URL/products/$PRODUCT_ID Step 9: List Products by Category curl \u0026#34;$API_URL/products?category=electronics\u0026#34; Step 10: Create Order curl -X POST $API_URL/orders \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -d \u0026#39;{ \u0026#34;items\u0026#34;: [ { \u0026#34;product_id\u0026#34;: \u0026#34;product-id-here\u0026#34;, \u0026#34;quantity\u0026#34;: 2 } ] }\u0026#39; Expected response:\n{ \u0026#34;order_id\u0026#34;: \u0026#34;uuid-here\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;user-id-here\u0026#34;, \u0026#34;items\u0026#34;: [...], \u0026#34;total_amount\u0026#34;: 1999.98, \u0026#34;status\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-01-01T00:00:00Z\u0026#34; } Step 11: List User Orders curl -X GET $API_URL/orders \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; Step 12: Get Order by ID ORDER_ID=\u0026#34;your-order-id\u0026#34; curl -X GET $API_URL/orders/$ORDER_ID \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; Testing in Browser You can test public endpoints directly in browser:\nHealth: https://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com/health Products: https://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com/products Swagger: https://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com/docs View Resources in AWS Console Lambda Function Navigate to Lambda Console Find: fastapi-lambda-fn View configuration, logs, and metrics API Gateway Navigate to API Gateway Console Find your API (name: fastapi-lambda-http) View routes, stages, and logs DynamoDB Tables Navigate to DynamoDB Console View tables: products, orders, users Explore table items and indexes CloudWatch Logs Navigate to CloudWatch Logs Find: /aws/lambda/fastapi-lambda-fn View logs with correlation IDs Common Test Scenarios Authentication Flow # Register → Login → Use Token curl -X POST $API_URL/auth/register \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;email\u0026#34;:\u0026#34;test@test.com\u0026#34;,\u0026#34;password\u0026#34;:\u0026#34;pass123\u0026#34;}\u0026#39; TOKEN=$(curl -s -X POST $API_URL/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;email\u0026#34;:\u0026#34;test@test.com\u0026#34;,\u0026#34;password\u0026#34;:\u0026#34;pass123\u0026#34;}\u0026#39; \\ | jq -r \u0026#39;.access_token\u0026#39;) curl -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; $API_URL/products Error Handling # Invalid credentials curl -X POST $API_URL/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;email\u0026#34;:\u0026#34;wrong@test.com\u0026#34;,\u0026#34;password\u0026#34;:\u0026#34;wrong\u0026#34;}\u0026#39; # Missing token curl $API_URL/orders # Invalid token curl -H \u0026#34;Authorization: Bearer invalid-token\u0026#34; $API_URL/orders Use correlation IDs from response headers (X-Correlation-ID) to trace requests across CloudWatch logs.\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"DevOps on AWS Workshop Event Information Date \u0026amp; Time: Monday, November 17, 2025, 8:30 AM – 5:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent Purpose The workshop was designed to provide comprehensive knowledge and hands-on experience with AWS DevOps services, covering CI/CD pipelines, Infrastructure as Code, container services, and monitoring \u0026amp; observability. The event aimed to help participants understand DevOps culture, principles, and best practices while exploring practical implementation of DevOps workflows on AWS.\nAgenda Overview Morning Session (8:30 AM – 12:00 PM) 8:30 – 9:00 AM | Welcome \u0026amp; DevOps Mindset\nRecap of AI/ML session from previous workshop DevOps Culture and Principles: Understanding the cultural shift from traditional IT to DevOps, emphasizing collaboration, automation, and continuous improvement Benefits and Key Metrics: DORA Metrics: Deployment frequency, lead time for changes, mean time to recovery (MTTR), change failure rate MTTR (Mean Time To Recovery): Measuring how quickly teams can recover from failures Deployment Frequency: Tracking how often teams deploy code to production Discussion on how DevOps practices improve software delivery and operational performance 9:00 – 10:30 AM | AWS DevOps Services – CI/CD Pipeline\nSource Control: AWS CodeCommit, Git Strategies\nAWS CodeCommit: Fully managed source control service, secure Git repositories Git Strategies: GitFlow: Feature branches, develop, release branches workflow Trunk-based Development: Main branch focused, short-lived feature branches Best practices for branching strategies based on team size and project requirements Build \u0026amp; Test: CodeBuild Configuration, Testing Pipelines\nAWS CodeBuild: Fully managed build service that compiles source code, runs tests, and produces ready-to-deploy software packages Build Configuration: Buildspec files, environment variables, and build artifacts Testing Pipelines: Unit tests, integration tests, and automated test execution Integration with testing frameworks and code quality tools Deployment: CodeDeploy with Blue/Green, Canary, and Rolling Updates\nAWS CodeDeploy: Automated application deployments to EC2, Lambda, or on-premises servers Blue/Green Deployment: Zero-downtime deployment by running two identical production environments Canary Deployment: Gradual rollout to a small percentage of users before full deployment Rolling Updates: Incremental deployment across instances with automatic rollback capabilities Choosing the right deployment strategy based on application requirements Orchestration: CodePipeline Automation\nAWS CodePipeline: Fully managed continuous delivery service for automating release pipelines Pipeline Stages: Source, Build, Test, Deploy, and Approval stages Integration: Connecting CodeCommit, CodeBuild, CodeDeploy, and other AWS services Automation: Automated triggers, parallel execution, and pipeline visualization Demo: Full CI/CD Pipeline Walkthrough\nThe demonstration showcased a complete CI/CD pipeline:\nSetting up CodeCommit repository Configuring CodeBuild for automated builds and tests Creating CodeDeploy application with Blue/Green deployment Building CodePipeline to orchestrate the entire workflow Testing the pipeline with code changes and observing automated deployment 10:30 – 10:45 AM | Break\nNetworking and refreshments.\n10:45 AM – 12:00 PM | Infrastructure as Code (IaC)\nAWS CloudFormation: Templates, Stacks, and Drift Detection\nCloudFormation Templates: JSON/YAML templates for defining AWS resources Stacks: Collections of AWS resources managed as a single unit Drift Detection: Identifying changes made outside of CloudFormation Stack Updates: Updating infrastructure with change sets and rollback capabilities Best Practices: Template organization, parameterization, and nested stacks AWS CDK (Cloud Development Kit): Constructs, Reusable Patterns, and Language Support\nAWS CDK: Define cloud infrastructure using familiar programming languages (TypeScript, Python, Java, C#, Go) Constructs: Reusable cloud components, from low-level resources to high-level patterns Reusable Patterns: Pre-built solutions for common use cases (VPC, ECS clusters, serverless applications) Language Support: TypeScript, Python, Java, C#, Go, and JavaScript Benefits: Type safety, IDE support, and easier testing compared to CloudFormation templates Demo: Deploying with CloudFormation and CDK\nThe demonstration compared both approaches:\nCloudFormation: Deploying a VPC and EC2 instance using YAML template CDK: Same infrastructure using TypeScript with CDK constructs Highlighting the differences in approach, maintainability, and developer experience Discussion: Choosing between IaC Tools\nWhen to use CloudFormation vs CDK Considerations: team expertise, project complexity, and maintenance requirements Hybrid approaches: Using both tools together for different parts of infrastructure Lunch Break (12:00 – 1:00 PM)\nSelf-arranged lunch break.\nAfternoon Session (1:00 – 5:00 PM) 1:00 – 2:30 PM | Container Services on AWS\nDocker Fundamentals: Microservices and Containerization\nContainerization Concepts: Understanding containers, images, and containerization benefits Microservices Architecture: Breaking monolithic applications into smaller, independent services Docker Basics: Dockerfile, image building, container lifecycle Benefits: Portability, consistency, and resource efficiency Amazon ECR: Image Storage, Scanning, Lifecycle Policies\nAmazon ECR: Fully managed Docker container registry Image Storage: Secure, scalable storage for Docker images Image Scanning: Automated vulnerability scanning for container images Lifecycle Policies: Automating image cleanup and retention policies Integration: Seamless integration with ECS, EKS, and other AWS services Amazon ECS \u0026amp; EKS: Deployment Strategies, Scaling, and Orchestration\nAmazon ECS: Fully managed container orchestration service\nTask Definitions: Container specifications, resource requirements, and networking Services: Long-running tasks with load balancing and auto-scaling Deployment Strategies: Rolling updates, Blue/Green deployments Scaling: Auto-scaling based on CPU, memory, or custom metrics Amazon EKS: Managed Kubernetes service\nKubernetes Concepts: Pods, Services, Deployments, and Namespaces EKS Features: Managed control plane, node groups, and add-ons Deployment Strategies: Rolling updates, Canary deployments with Istio/App Mesh Scaling: Cluster autoscaler and horizontal pod autoscaler AWS App Runner: Simplified Container Deployment\nApp Runner: Fully managed service for building and running containerized applications Simplified Deployment: Deploy from source code or container image Auto-scaling: Automatic scaling based on traffic Use Cases: Web applications, APIs, and microservices Comparison: When to use App Runner vs ECS vs EKS Demo \u0026amp; Case Study: Microservices Deployment Comparison\nThe demonstration compared different container deployment options:\nDeploying a simple web application with App Runner Same application with ECS Fargate Comparison of setup complexity, cost, and operational overhead Case study: Choosing the right container service for different scenarios 2:30 – 2:45 PM | Break\nNetworking and refreshments.\n2:45 – 4:00 PM | Monitoring \u0026amp; Observability\nCloudWatch: Metrics, Logs, Alarms, and Dashboards\nCloudWatch Metrics: Collecting and tracking metrics from AWS services and custom applications CloudWatch Logs: Centralized logging, log groups, and log streams CloudWatch Alarms: Automated actions based on metric thresholds CloudWatch Dashboards: Customizable dashboards for visualizing metrics and logs Best Practices: Metric naming conventions, log retention, and alarm configuration AWS X-Ray: Distributed Tracing and Performance Insights\nAWS X-Ray: Service for analyzing and debugging distributed applications Distributed Tracing: End-to-end request tracing across microservices Service Map: Visual representation of application architecture and dependencies Performance Insights: Identifying bottlenecks and performance issues Integration: X-Ray SDK integration with applications and AWS services Demo: Full-Stack Observability Setup\nThe demonstration showed:\nSetting up CloudWatch metrics and logs for an application Creating CloudWatch dashboards for monitoring Configuring CloudWatch alarms for alerting Enabling X-Ray tracing for distributed tracing Viewing service maps and trace analysis Best Practices: Alerting, Dashboards, and On-Call Processes\nAlerting Strategy: Setting up meaningful alerts, avoiding alert fatigue Dashboard Design: Creating effective dashboards for different audiences (developers, operations, management) On-Call Processes: Incident response procedures, escalation paths, and runbooks SLO/SLI: Service Level Objectives and Indicators for measuring reliability 4:00 – 4:45 PM | DevOps Best Practices \u0026amp; Case Studies\nDeployment Strategies: Feature Flags, A/B Testing\nFeature Flags: Gradual feature rollouts, canary releases, and instant rollbacks A/B Testing: Comparing different versions to optimize user experience Tools: AWS AppConfig, LaunchDarkly integration Best Practices: Feature flag management and testing strategies Automated Testing and CI/CD Integration\nTesting Pyramid: Unit tests, integration tests, and end-to-end tests Test Automation: Automated test execution in CI/CD pipelines Quality Gates: Blocking deployments based on test results Test Coverage: Measuring and improving test coverage Incident Management and Postmortems\nIncident Response: Detection, response, and recovery procedures Postmortems: Learning from incidents, documenting root causes Blameless Culture: Focusing on system improvements rather than individual blame Tools: Incident management tools and communication channels Case Studies: Startups and Enterprise DevOps Transformations\nStartup Case Study: Rapid scaling with DevOps practices, cost optimization Enterprise Case Study: Large-scale migration to DevOps, cultural transformation Lessons Learned: Common challenges and solutions ROI: Measuring the impact of DevOps adoption 4:45 – 5:00 PM | Q\u0026amp;A \u0026amp; Wrap-up\nDevOps Career Pathways: Career progression in DevOps, required skills AWS Certification Roadmap: Relevant AWS certifications for DevOps engineers AWS Certified DevOps Engineer – Professional AWS Certified Solutions Architect AWS Certified SysOps Administrator Next Steps: Resources for continued learning and practice Closing Remarks: Summary of key takeaways and action items Key Highlights CI/CD Pipeline: AWS CodePipeline provides a complete solution for automating software delivery from source to production Infrastructure as Code: Both CloudFormation and CDK offer powerful ways to manage infrastructure, with CDK providing better developer experience Container Services: AWS offers multiple container options (ECS, EKS, App Runner) for different use cases and complexity levels Observability: CloudWatch and X-Ray together provide comprehensive monitoring and tracing capabilities DevOps Culture: Success requires cultural change, not just tools and technology Best Practices: Feature flags, automated testing, and incident management are essential for modern DevOps Key Learnings DevOps is Culture First: Tools are important, but cultural transformation is the foundation of DevOps success CI/CD Automation: Automating the entire software delivery pipeline significantly improves speed and reliability IaC Benefits: Infrastructure as Code enables version control, repeatability, and faster infrastructure changes Container Strategy: Choosing the right container service depends on complexity, team expertise, and operational requirements Observability is Critical: Comprehensive monitoring and tracing are essential for maintaining reliable systems Continuous Improvement: DevOps is about continuous learning and improvement, not a one-time implementation Application to My Work Implement CI/CD: Set up CodePipeline for automated deployments in current projects Adopt IaC: Start using CloudFormation or CDK for infrastructure management Container Migration: Evaluate containerization opportunities for existing applications Improve Monitoring: Enhance CloudWatch dashboards and alarms for better visibility Practice DevOps: Apply DevOps principles and practices in daily work Incident Management: Establish incident response procedures and postmortem practices Personal Experience This full-day DevOps workshop was comprehensive and highly practical:\nThe CI/CD pipeline demonstration was particularly valuable, showing how to automate the entire software delivery process Learning about different IaC tools helped me understand when to use CloudFormation vs CDK The container services comparison provided clear guidance on choosing the right service for different scenarios The observability session emphasized the importance of monitoring and tracing for maintaining reliable systems The case studies provided real-world insights into DevOps transformations The career pathway discussion was motivating and provided clear direction for professional development Takeaways Start Small: Begin with basic CI/CD automation and gradually expand DevOps practices Culture Matters: DevOps success requires team collaboration and cultural change Choose the Right Tools: Select tools based on team expertise and project requirements Monitor Everything: Comprehensive observability is essential for reliable systems Learn Continuously: DevOps practices evolve rapidly, requiring continuous learning Measure Success: Use DORA metrics to track DevOps improvements and ROI Event Photos "},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives (per CloudJourney roadmap) Expand databases: NoSQL DynamoDB, caching layers (Databases in Module 1). Enhance observability: Auditing with CloudTrail, tracing with X-Ray (Monitoring). Initial migration assessment tools (Transition to Module 2). Consolidate foundational services. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Databases — NoSQL: Design DynamoDB tables, provisioned vs on-demand capacity, global tables. 29/09/2025 29/09/2025 https://cloudjourney.awsstudygroup.com/ - Explore AWS Services Tuesday Labs: Create DynamoDB table, load data via Lambda, query with PartiQL. 30/09/2025 30/09/2025 https://cloudjourney.awsstudygroup.com/ - Databases Labs Wednesday Caching: Set up ElastiCache Redis, integrate with RDS/DynamoDB for read-heavy apps. 01/10/2025 01/10/2025 https://cloudjourney.awsstudygroup.com/ - Databases Thursday Monitoring advanced: Enable CloudTrail trails, integrate X-Ray with Lambda/ECS. 02/10/2025 02/10/2025 https://cloudjourney.awsstudygroup.com/ - Monitoring Friday Module 2 intro: Workload assessment using Migration Evaluator, plan simple lift-and-shift. 03/10/2025 03/10/2025 https://cloudjourney.awsstudygroup.com/ - Migrate to AWS Results \u0026amp; Achievements (Week 4) DynamoDB mastered: Tables created with secondary indexes, data ingested, queries optimized for performance. Caching layer added: Redis cluster reducing RDS load by 70% in test scenarios. Observability stack complete: CloudTrail logging API calls, X-Ray traces showing end-to-end latencies. Migration planning initiated: Assessed sample on-prem workload, identified EC2 equivalents. Foundational services consolidated: Full stack (network-storage-compute-db-monitoring) documented. Issues Encountered \u0026amp; Mitigations DynamoDB provisioned capacity over-provisioned → switched to on-demand and monitored with CloudWatch. X-Ray sampling rate too high → adjusted to 10% for cost control. Migration Evaluator data upload slow → used direct connector for faster assessment. Next Steps (Week 5) Dive into Module 2: Server and application migration strategies. Database migration with DMS. Network connectivity for hybrid setups. Review foundational knowledge with a mini-project. "},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025: Ho Chi Minh Connect Edition for Builders\nDate \u0026amp; Time: 08:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GAI-Driven Development Life Cycle: Reimagining Software Engineering.\nDate \u0026amp; Time: 14:00, October 3, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AI/ML/GenAI on AWS Workshop\nDate \u0026amp; Time: Saturday, November 15, 2025, 8:30 AM – 12:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent 4 Event Name: DevOps on AWS Workshop\nDate \u0026amp; Time: Monday, November 17, 2025, 8:30 AM – 5:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent 5 Event Name: AWS Well-Architected Security Pillar Workshop\nDate \u0026amp; Time: Saturday, November 29, 2025, 8:30 AM – 12:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/5-workshop/5.4-s3-onprem/","title":"FastAPI Backend Deployment","tags":[],"description":"","content":"FastAPI Backend Deployment The FastAPI backend is automatically deployed by the GitLab CI/CD pipeline. This section explains the application structure and how to test it.\nArchitecture Overview The application follows a clean layered architecture:\nLayer Location Responsibility API app/api/routers/ HTTP request handling, routing Service app/services/ Business logic, orchestration Repository app/repositories/ Data access, DynamoDB operations Models app/models/ Data structures, validation Core app/core/ Configuration, security, logging Request Flow HTTP Request ↓ API Gateway HTTP API ↓ Lambda Function (Container) ↓ Mangum (ASGI Adapter) ↓ FastAPI Application ├── /auth (register, login) ├── /products (CRUD operations) └── /orders (create, list, get) ↓ Service Layer (Business Logic) ↓ Repository Layer (DynamoDB) ↓ DynamoDB Tables Content FastAPI Application Structure - Code structure and architecture Infrastructure Deployment - Terraform modules and AWS resources Testing the API - Test endpoints with curl and Swagger UI Load Testing - Performance testing with Artillery Automatic Deployment The terraform_deploy job in GitLab CI/CD automatically:\nInitializes Terraform Plans infrastructure changes Applies changes (creates/updates resources) Outputs API Gateway URL No manual deployment needed! Just push code and the pipeline handles everything.\nInfrastructure Created DynamoDB Tables: products, orders, users Lambda Function: Container-based FastAPI application API Gateway: HTTP API (v2) with Lambda integration IAM Roles: Least privilege permissions CloudWatch: Log groups and alarms SNS: Alert notifications Get API URL After pipeline completes:\n# From GitLab job logs (terraform_deploy) # Or from Terraform output: cd Backend-FastAPI-Docker_Build-Pipeline/infra terraform output api_url Test Health Endpoint API_URL=\u0026#34;https://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com\u0026#34; curl $API_URL/health # Expected: {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;} API Endpoints Method Endpoint Description Auth GET /health Health check Public POST /auth/register Register user Public POST /auth/login Login and get JWT Public GET /products List products Public POST /products Create product Admin GET /products/{id} Get product Public PUT /products/{id} Update product Admin DELETE /products/{id} Delete product Admin GET /orders List user orders User POST /orders Create order User GET /orders/{id} Get order User Swagger Documentation Access interactive API documentation:\nhttps://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com/docs The backend is automatically deployed by the pipeline. You only need to test it after deployment completes.\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"Load Testing with Artillery","tags":[],"description":"","content":"Load Testing with Artillery In this section, you will perform load testing on the deployed API using Artillery to simulate user traffic and measure performance.\nPrerequisites Install Artillery:\nnpm install -g artillery Or using Docker:\ndocker run -it --rm -v $(pwd):/work artilleryio/artillery:latest Step 1: Get API URL and Token # Get API URL cd Backend-FastAPI-Docker_Build-Pipeline/infra API_URL=$(terraform output -raw api_url) # Get admin token (you need to login first) TOKEN=\u0026#34;YOUR_ADMIN_TOKEN_HERE\u0026#34; Note: Replace with your actual admin token from /auth/login endpoint.\nStep 2: Create Load Test Configuration Create load-test.yml:\nconfig: target: \u0026#34;https://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com\u0026#34; phases: - duration: 60 arrivalRate: 10 name: \u0026#34;Warm up\u0026#34; - duration: 120 arrivalRate: 50 name: \u0026#34;Ramp up\u0026#34; - duration: 60 arrivalRate: 10 name: \u0026#34;Cool down\u0026#34; defaults: headers: Authorization: \u0026#34;Bearer YOUR_ADMIN_TOKEN_HERE\u0026#34; Content-Type: \u0026#34;application/json\u0026#34; scenarios: - name: \u0026#34;Get products\u0026#34; weight: 70 flow: - get: url: \u0026#34;/products\u0026#34; - name: \u0026#34;Create product\u0026#34; weight: 30 flow: - post: url: \u0026#34;/products\u0026#34; json: name: \u0026#34;Load test product {{ $randomString() }}\u0026#34; price: 29.99 stock: 100 category: \u0026#34;electronics\u0026#34; Important: Replace:\ntarget with your actual API Gateway URL Authorization header with your admin token Step 3: Run Load Test artillery run load-test.yml Step 4: Analyze Results Artillery will output statistics like:\nSummary report @ 14:30:00(+0000) 2024-01-01 Scenarios launched: 1000 Scenarios completed: 1000 Requests completed: 2000 Mean response/sec: 45.2 Response time (msec): min: 45 max: 1200 median: 120 p95: 350 p99: 800 Scenario counts: Get products: 700 (70%) Create product: 300 (30%) Codes: 200: 1950 201: 50 500: 0 Step 5: Generate HTML Report artillery run --output report.json load-test.yml artillery report report.json This generates an HTML report with:\nResponse time graphs Request rate charts Error rates Scenario breakdown Advanced Load Test Configuration For more realistic testing:\nconfig: target: \u0026#34;https://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com\u0026#34; phases: - duration: 120 arrivalRate: 5 name: \u0026#34;Warm up\u0026#34; - duration: 300 arrivalRate: 20 name: \u0026#34;Sustained load\u0026#34; - duration: 60 arrivalRate: 5 name: \u0026#34;Cool down\u0026#34; defaults: headers: Authorization: \u0026#34;Bearer YOUR_TOKEN\u0026#34; Content-Type: \u0026#34;application/json\u0026#34; processor: \u0026#34;./processor.js\u0026#34; # Custom processor for dynamic data scenarios: - name: \u0026#34;Product browsing\u0026#34; weight: 60 flow: - get: url: \u0026#34;/products\u0026#34; - think: 2 # Wait 2 seconds - get: url: \u0026#34;/products?category=electronics\u0026#34; - name: \u0026#34;Product management\u0026#34; weight: 30 flow: - post: url: \u0026#34;/products\u0026#34; json: name: \u0026#34;Product {{ $randomString() }}\u0026#34; price: {{ $randomNumber(10, 1000) }} stock: {{ $randomNumber(1, 100) }} category: \u0026#34;{{ $pick([\u0026#39;electronics\u0026#39;, \u0026#39;clothing\u0026#39;, \u0026#39;books\u0026#39;]) }}\u0026#34; - think: 1 - get: url: \u0026#34;/products\u0026#34; - name: \u0026#34;Order flow\u0026#34; weight: 10 flow: - get: url: \u0026#34;/products\u0026#34; - post: url: \u0026#34;/orders\u0026#34; json: items: - product_id: \u0026#34;{{ $pick([\u0026#39;id1\u0026#39;, \u0026#39;id2\u0026#39;, \u0026#39;id3\u0026#39;]) }}\u0026#34; quantity: {{ $randomNumber(1, 5) }} Monitor During Load Test CloudWatch Metrics Navigate to CloudWatch → Metrics → AWS/Lambda Select fastapi-lambda-fn Monitor: Invocations: Request count Duration: Response time Errors: Error rate Throttles: Throttled requests CloudWatch Logs # Watch logs in real-time aws logs tail /aws/lambda/fastapi-lambda-fn --follow --region ap-southeast-1 Performance Benchmarks Expected performance for Lambda container:\nMetric Target Notes P50 (median) \u0026lt; 200ms Most requests P95 \u0026lt; 500ms 95% of requests P99 \u0026lt; 1000ms 99% of requests Error Rate \u0026lt; 0.1% Very low errors Throughput 50+ req/s Depends on concurrency Troubleshooting High Response Times Check Lambda cold starts Monitor DynamoDB throttling Review CloudWatch metrics Check API Gateway latency High Error Rates Check CloudWatch logs for errors Verify DynamoDB table capacity Check Lambda timeout settings Review API Gateway limits Throttling Increase Lambda concurrency limit Check DynamoDB on-demand capacity Review API Gateway throttling settings Best Practices Start Small: Begin with low arrival rates and gradually increase Monitor Resources: Watch CloudWatch metrics during testing Test Realistic Scenarios: Mix read and write operations Use Correlation IDs: Track requests across logs Test Error Handling: Include negative test cases Run load tests during off-peak hours and monitor AWS costs. Load testing can generate significant traffic and incur costs.\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"AWS Well-Architected Security Pillar Workshop Event Information Date \u0026amp; Time: Saturday, November 29, 2025, 8:30 AM – 12:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent Purpose This morning workshop provided a comprehensive deep-dive into the AWS Well-Architected Security Pillar, covering all five security domains: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response.\nThe session was designed to equip participants with practical knowledge on implementing security best practices in AWS environments, with real-world examples from Vietnamese enterprises.\nAgenda 8:30 – 8:50 AM | Opening \u0026amp; Security Foundation Security Pillar in Well-Architected Framework\nRole of Security Pillar in the Well-Architected Framework Core principles: Least Privilege – Zero Trust – Defense in Depth AWS Shared Responsibility Model Top cloud security threats in Vietnam Pillar 1 — Identity \u0026amp; Access Management 8:50 – 9:30 AM | Modern IAM Architecture\nIAM fundamentals: Users, Roles, Policies – avoiding long-term credentials IAM Identity Center: SSO, permission sets SCP \u0026amp; Permission Boundaries for multi-account environments MFA, credential rotation, Access Analyzer Mini Demo: Validate IAM Policy + simulate access Pillar 2 — Detection 9:30 – 9:55 AM | Detection \u0026amp; Continuous Monitoring\nCloudTrail (org-level), GuardDuty, Security Hub Logging at every layer: VPC Flow Logs, ALB/S3 logs Alerting \u0026amp; automation with EventBridge Detection-as-Code (infrastructure + rules) 9:55 – 10:10 AM | Coffee Break Pillar 3 — Infrastructure Protection 10:10 – 10:40 AM | Network \u0026amp; Workload Security\nVPC segmentation, private vs public placement Security Groups vs NACLs: application models WAF + Shield + Network Firewall Workload protection: EC2, ECS/EKS security basics Pillar 4 — Data Protection 10:40 – 11:10 AM | Encryption, Keys \u0026amp; Secrets\nKMS: key policies, grants, rotation Encryption at-rest \u0026amp; in-transit: S3, EBS, RDS, DynamoDB Secrets Manager \u0026amp; Parameter Store — rotation patterns Data classification \u0026amp; access guardrails Pillar 5 — Incident Response 11:10 – 11:40 AM | IR Playbook \u0026amp; Automation\nIR lifecycle according to AWS framework Playbooks: Compromised IAM key S3 public exposure EC2 malware detection Snapshot, isolation, evidence collection Auto-response with Lambda/Step Functions 11:40 – 12:00 PM | Wrap-Up \u0026amp; Q\u0026amp;A Summary of 5 security pillars Common pitfalls \u0026amp; real-world practices in Vietnamese enterprises Security learning roadmap (Security Specialty, SA Pro certifications) Key Learnings Least Privilege is fundamental – always start with minimal permissions and expand as needed Zero Trust architecture assumes no implicit trust, even within the network Defense in Depth requires security controls at multiple layers Detection capabilities must be automated and continuous Incident response playbooks should be documented and tested regularly Application to My Work Implement IAM Access Analyzer in current projects Set up GuardDuty and Security Hub for centralized security monitoring Create incident response playbooks for common scenarios Review and tighten Security Group rules using least privilege Enable encryption for all data stores (S3, RDS, DynamoDB) Personal Experience This workshop was incredibly valuable for understanding AWS security holistically:\nThe practical demos made abstract concepts tangible Learning about common security pitfalls in Vietnamese enterprises was eye-opening The incident response playbooks provided actionable templates Understanding the relationship between all five pillars helps in designing comprehensive security architectures Takeaways Security is a continuous journey, not a destination Automation is key to maintaining security at scale The Well-Architected Security Pillar provides a comprehensive framework for cloud security Regular security reviews and improvements are essential AWS provides powerful native tools for each security domain Event Photos "},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives (per CloudJourney roadmap) Server migration: Using AWS MGN and Server Migration Service (Module 2). Application migration patterns: Lift-and-shift to EC2. Hands-on replication and cutover. Update cost management with migrated resources. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 2 — Server migration: Study AWS MGN setup, replication servers, test migrations. 06/10/2025 06/10/2025 https://cloudjourney.awsstudygroup.com/ - Migrate to AWS Tuesday Labs: Launch MGN replication agent, migrate sample VM to EC2, validate post-launch. 07/10/2025 07/10/2025 https://cloudjourney.awsstudygroup.com/ - Module 2 Labs Wednesday Application lift-and-shift: Package app with SMS, deploy to EC2, configure dependencies. 08/10/2025 08/10/2025 https://cloudjourney.awsstudygroup.com/ - Migrate to AWS Thursday Cutover practice: Perform test cutover, DNS updates, rollback simulation. 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ - Module 2 Friday Cost review: Analyze migrated resources with Cost Explorer, apply savings plans. 10/10/2025 10/10/2025 https://cloudjourney.awsstudygroup.com/ - Cost Management Results \u0026amp; Achievements (Week 5) Successful VM migration: MGN replicated Windows/Linux servers to EC2, tested applications post-cutover. Lift-and-shift completed: Legacy app running on EC2 with minimal changes, dependencies resolved. Rollback procedures documented: Dry-run cutover with traffic switchback in under 5 minutes. Cost optimizations applied: Reserved Instances purchased for steady-state EC2 fleet. Mini-project reviewed: End-to-end migration of a sample workload from \u0026ldquo;on-prem\u0026rdquo; to AWS. Issues Encountered \u0026amp; Mitigations MGN agent connectivity issues → configured outbound rules in on-prem firewall simulation. App dependencies failed post-migration → used SSM for automated patching. Cutover DNS propagation delay → implemented Route 53 health checks for faster failover. Next Steps (Week 6) Database migration with AWS DMS. Hybrid network connectivity: Direct Connect/VPN. Post-migration validation and optimization. Prepare for optimization module. "},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/5-workshop/5.5-policy/","title":"Manual Configuration: Monitoring &amp; Security","tags":[],"description":"","content":"Manual Configuration: Monitoring \u0026amp; Security This section provides step-by-step manual configuration of CloudWatch monitoring, alarms, SNS notifications, and security settings for the FastAPI Backend.\nStep 1: Manual CloudWatch Log Group Configuration Create Log Group # Create CloudWatch log group for Lambda aws logs create-log-group \\ --log-group-name /aws/lambda/fastapi-lambda-fn \\ --region ap-southeast-1 # Set retention period (14 days) aws logs put-retention-policy \\ --log-group-name /aws/lambda/fastapi-lambda-fn \\ --retention-in-days 14 \\ --region ap-southeast-1 View Logs # List log streams aws logs describe-log-streams \\ --log-group-name /aws/lambda/fastapi-lambda-fn \\ --region ap-southeast-1 \\ --order-by LastEventTime \\ --descending \\ --max-items 10 # Get recent log events aws logs tail /aws/lambda/fastapi-lambda-fn \\ --follow \\ --region ap-southeast-1 CloudWatch Insights Query # Query logs using CloudWatch Insights aws logs start-query \\ --log-group-name /aws/lambda/fastapi-lambda-fn \\ --start-time $(date -d \u0026#39;1 hour ago\u0026#39; +%s) \\ --end-time $(date +%s) \\ --query-string \u0026#39;fields @timestamp, @message, correlation_id | filter @message like /error/ | sort @timestamp desc | limit 20\u0026#39; \\ --region ap-southeast-1 # Get query results (replace QUERY_ID) aws logs get-query-results \\ --query-id QUERY_ID \\ --region ap-southeast-1 Step 2: Manual CloudWatch Metrics Configuration View Lambda Metrics # List available metrics aws cloudwatch list-metrics \\ --namespace AWS/Lambda \\ --dimensions Name=FunctionName,Value=fastapi-lambda-fn \\ --region ap-southeast-1 # Get metric statistics aws cloudwatch get-metric-statistics \\ --namespace AWS/Lambda \\ --metric-name Invocations \\ --dimensions Name=FunctionName,Value=fastapi-lambda-fn \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 300 \\ --statistics Sum \\ --region ap-southeast-1 Create Custom Metric # In your FastAPI application import boto3 cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;, region_name=\u0026#39;ap-southeast-1\u0026#39;) # Publish custom metric cloudwatch.put_metric_data( Namespace=\u0026#39;FastAPI/Products\u0026#39;, MetricData=[ { \u0026#39;MetricName\u0026#39;: \u0026#39;ProductsCreated\u0026#39;, \u0026#39;Value\u0026#39;: 1, \u0026#39;Unit\u0026#39;: \u0026#39;Count\u0026#39;, \u0026#39;Timestamp\u0026#39;: datetime.utcnow() } ] ) Step 3: Manual CloudWatch Alarm Configuration Create SNS Topic First # Create SNS topic for alerts aws sns create-topic \\ --name fastapi-lambda-alerts \\ --region ap-southeast-1 # Get topic ARN SNS_TOPIC_ARN=$(aws sns list-topics \\ --region ap-southeast-1 \\ --query \u0026#39;Topics[?contains(TopicArn, `fastapi-lambda-alerts`)].TopicArn\u0026#39; \\ --output text) echo \u0026#34;SNS Topic ARN: $SNS_TOPIC_ARN\u0026#34; Create CloudWatch Alarm for Lambda Errors # Create alarm for Lambda errors aws cloudwatch put-metric-alarm \\ --alarm-name fastapi-lambda-5xx \\ --alarm-description \u0026#34;Alert when Lambda errors exceed threshold\u0026#34; \\ --metric-name Errors \\ --namespace AWS/Lambda \\ --statistic Sum \\ --period 60 \\ --evaluation-periods 1 \\ --threshold 1 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=FunctionName,Value=fastapi-lambda-fn \\ --alarm-actions $SNS_TOPIC_ARN \\ --region ap-southeast-1 Create Alarm for High Duration # Create alarm for Lambda duration aws cloudwatch put-metric-alarm \\ --alarm-name fastapi-lambda-duration \\ --alarm-description \u0026#34;Alert when Lambda duration is high\u0026#34; \\ --metric-name Duration \\ --namespace AWS/Lambda \\ --statistic Average \\ --period 300 \\ --evaluation-periods 2 \\ --threshold 5000 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=FunctionName,Value=fastapi-lambda-fn \\ --alarm-actions $SNS_TOPIC_ARN \\ --region ap-southeast-1 Create Alarm for Throttles # Create alarm for Lambda throttles aws cloudwatch put-metric-alarm \\ --alarm-name fastapi-lambda-throttles \\ --alarm-description \u0026#34;Alert when Lambda is throttled\u0026#34; \\ --metric-name Throttles \\ --namespace AWS/Lambda \\ --statistic Sum \\ --period 60 \\ --evaluation-periods 1 \\ --threshold 1 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=FunctionName,Value=fastapi-lambda-fn \\ --alarm-actions $SNS_TOPIC_ARN \\ --region ap-southeast-1 View Alarms # List all alarms aws cloudwatch describe-alarms \\ --alarm-name-prefix fastapi-lambda \\ --region ap-southeast-1 # Get alarm state aws cloudwatch describe-alarms \\ --alarm-names fastapi-lambda-5xx \\ --region ap-southeast-1 \\ --query \u0026#39;MetricAlarms[0].StateValue\u0026#39; \\ --output text Step 4: Manual SNS Subscription Configuration Subscribe Email # Subscribe email to SNS topic aws sns subscribe \\ --topic-arn $SNS_TOPIC_ARN \\ --protocol email \\ --notification-endpoint your-email@example.com \\ --region ap-southeast-1 # Check your email and confirm subscription Subscribe SMS # Subscribe SMS to SNS topic aws sns subscribe \\ --topic-arn $SNS_TOPIC_ARN \\ --protocol sms \\ --notification-endpoint +1234567890 \\ --region ap-southeast-1 List Subscriptions # List all subscriptions aws sns list-subscriptions-by-topic \\ --topic-arn $SNS_TOPIC_ARN \\ --region ap-southeast-1 Test Notification # Publish test message aws sns publish \\ --topic-arn $SNS_TOPIC_ARN \\ --message \u0026#34;Test alert from FastAPI Lambda\u0026#34; \\ --subject \u0026#34;Test Alert\u0026#34; \\ --region ap-southeast-1 Step 5: Manual Security Configuration Enable ECR Image Scanning # Enable scanning on existing repository aws ecr put-image-scanning-configuration \\ --repository-name fastapi-lambda \\ --image-scanning-configuration scanOnPush=true \\ --region ap-southeast-1 # Verify scanning is enabled aws ecr describe-repositories \\ --repository-names fastapi-lambda \\ --region ap-southeast-1 \\ --query \u0026#39;repositories[0].imageScanningConfiguration\u0026#39; Configure Secrets Manager Encryption # Create KMS key for Secrets Manager KMS_KEY_ID=$(aws kms create-key \\ --description \u0026#34;FastAPI JWT Secret Key\u0026#34; \\ --region ap-southeast-1 \\ --query \u0026#39;KeyMetadata.KeyId\u0026#39; \\ --output text) # Update secret to use KMS encryption aws secretsmanager update-secret \\ --secret-id fastapi-jwt-secret \\ --kms-key-id $KMS_KEY_ID \\ --region ap-southeast-1 Enable API Gateway Logging # Get API Gateway ID API_ID=$(aws apigatewayv2 get-apis \\ --region ap-southeast-1 \\ --query \u0026#39;Items[?Name==`fastapi-lambda-http`].ApiId\u0026#39; \\ --output text) # Create CloudWatch log group for API Gateway aws logs create-log-group \\ --log-group-name /aws/apigateway/fastapi-lambda \\ --region ap-southeast-1 # Enable access logging (requires IAM role with logs:CreateLogStream permission) aws apigatewayv2 update-stage \\ --api-id $API_ID \\ --stage-name \u0026#39;$default\u0026#39; \\ --access-log-settings DestinationArn=arn:aws:logs:ap-southeast-1:ACCOUNT_ID:log-group:/aws/apigateway/fastapi-lambda,Format=\u0026#39;{\u0026#34;requestId\u0026#34;:\u0026#34;$context.requestId\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;$context.identity.sourceIp\u0026#34;,\u0026#34;requestTime\u0026#34;:\u0026#34;$context.requestTime\u0026#34;,\u0026#34;httpMethod\u0026#34;:\u0026#34;$context.httpMethod\u0026#34;,\u0026#34;routeKey\u0026#34;:\u0026#34;$context.routeKey\u0026#34;,\u0026#34;status\u0026#34;:\u0026#34;$context.status\u0026#34;,\u0026#34;protocol\u0026#34;:\u0026#34;$context.protocol\u0026#34;,\u0026#34;responseLength\u0026#34;:\u0026#34;$context.responseLength\u0026#34;}\u0026#39; \\ --region ap-southeast-1 Step 6: Create Monitoring Dashboard Create CloudWatch Dashboard # Create dashboard JSON cat \u0026gt; dashboard.json \u0026lt;\u0026lt;EOF { \u0026#34;widgets\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;AWS/Lambda\u0026#34;, \u0026#34;Invocations\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Invocations\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;Errors\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Errors\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;Duration\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Duration (ms)\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;Throttles\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Throttles\u0026#34;}] ], \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Lambda Metrics\u0026#34;, \u0026#34;view\u0026#34;: \u0026#34;timeSeries\u0026#34;, \u0026#34;yAxis\u0026#34;: { \u0026#34;left\u0026#34;: { \u0026#34;min\u0026#34;: 0 } } } } ] } EOF # Create dashboard aws cloudwatch put-dashboard \\ --dashboard-name FastAPI-Monitoring \\ --dashboard-body file://dashboard.json \\ --region ap-southeast-1 Step 7: Verify Configuration Check All Resources # Verify log group exists aws logs describe-log-groups \\ --log-group-name-prefix /aws/lambda/fastapi-lambda \\ --region ap-southeast-1 # Verify alarms exist aws cloudwatch describe-alarms \\ --alarm-name-prefix fastapi-lambda \\ --region ap-southeast-1 # Verify SNS topic exists aws sns list-topics \\ --region ap-southeast-1 \\ --query \u0026#39;Topics[?contains(TopicArn, `fastapi-lambda-alerts`)]\u0026#39; # Verify SNS subscriptions aws sns list-subscriptions-by-topic \\ --topic-arn $SNS_TOPIC_ARN \\ --region ap-southeast-1 Complete Configuration Script Create configure-monitoring.sh:\n#!/bin/bash set -e REGION=\u0026#34;ap-southeast-1\u0026#34; FUNCTION_NAME=\u0026#34;fastapi-lambda-fn\u0026#34; PROJECT_NAME=\u0026#34;fastapi-lambda\u0026#34; echo \u0026#34;Configuring Monitoring \u0026amp; Security...\u0026#34; # 1. Create CloudWatch Log Group echo \u0026#34;Creating CloudWatch log group...\u0026#34; aws logs create-log-group \\ --log-group-name /aws/lambda/$FUNCTION_NAME \\ --region $REGION 2\u0026gt;/dev/null || echo \u0026#34;Log group already exists\u0026#34; aws logs put-retention-policy \\ --log-group-name /aws/lambda/$FUNCTION_NAME \\ --retention-in-days 14 \\ --region $REGION # 2. Create SNS Topic echo \u0026#34;Creating SNS topic...\u0026#34; SNS_TOPIC_ARN=$(aws sns create-topic \\ --name ${PROJECT_NAME}-alerts \\ --region $REGION \\ --query \u0026#39;TopicArn\u0026#39; \\ --output text) echo \u0026#34;SNS Topic ARN: $SNS_TOPIC_ARN\u0026#34; # 3. Create CloudWatch Alarms echo \u0026#34;Creating CloudWatch alarms...\u0026#34; aws cloudwatch put-metric-alarm \\ --alarm-name ${PROJECT_NAME}-5xx \\ --alarm-description \u0026#34;Alert when Lambda errors exceed threshold\u0026#34; \\ --metric-name Errors \\ --namespace AWS/Lambda \\ --statistic Sum \\ --period 60 \\ --evaluation-periods 1 \\ --threshold 1 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=FunctionName,Value=$FUNCTION_NAME \\ --alarm-actions $SNS_TOPIC_ARN \\ --region $REGION # 4. Subscribe Email (replace with your email) read -p \u0026#34;Enter your email for alerts: \u0026#34; EMAIL aws sns subscribe \\ --topic-arn $SNS_TOPIC_ARN \\ --protocol email \\ --notification-endpoint $EMAIL \\ --region $REGION echo \u0026#34;Monitoring configuration complete!\u0026#34; echo \u0026#34;Check your email to confirm SNS subscription\u0026#34; Run:\nchmod +x configure-monitoring.sh ./configure-monitoring.sh Best Practices Log Retention: Set appropriate retention (14-30 days) to balance cost and compliance Alarm Thresholds: Set based on expected error rates and business requirements Multiple Channels: Use Email, SMS, and Slack for critical alerts Regular Review: Review alarms and adjust thresholds based on actual metrics Cost Monitoring: Set up billing alerts to monitor costs Always test alarms by triggering them manually to ensure notifications work correctly.\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building a Secure FastAPI Backend on AWS Lambda with DevSecOps Overview This is a step-by-step deployment guide that will help you deploy a complete FastAPI Backend API running on AWS Lambda (container-based) with a fully automated CI/CD pipeline, integrated security scanning, and Infrastructure as Code using Terraform.\nProject Repository: https://gitlab.com/m.quang/devsecops-aws-ver2\nFollow this guide to deploy a production-ready serverless application with automated security scanning, monitoring, and infrastructure management.\nYou will learn how to:\nBuild a FastAPI application with clean layered architecture (API → Service → Repository) Package FastAPI as a Lambda container image using Mangum adapter Set up AWS CodePipeline with CodeBuild for automated CI/CD Integrate security scanning with Semgrep (SAST) and Trivy (container vulnerability scanning) Deploy infrastructure using Terraform modules Store data in DynamoDB tables (products, orders, users) Implement JWT authentication with AWS Secrets Manager Set up monitoring with CloudWatch and SNS alerts Architecture The architecture is divided into three main domains:\nCI/CD Pipeline Domain: GitLab → CodePipeline → CodeBuild (Semgrep → Docker Build → Trivy) → ECR → Terraform Deploy Application Domain: API Gateway HTTP API → Lambda (Container) → FastAPI → DynamoDB + Secrets Manager Monitoring Domain: CloudWatch Logs → CloudWatch Alarms → SNS Alerts Request Flow:\nUser Request ↓ API Gateway (HTTP API) ↓ Lambda Function (Container Image from ECR) ↓ Mangum (ASGI Adapter) ↓ FastAPI Application ├── /auth → JWT Authentication ├── /products → Product CRUD Operations └── /orders → Order CRUD Operations ↓ DynamoDB Tables ├── products (product_id, name, price, stock, category) ├── orders (order_id, user_id, products, status) └── users (user_id, email, password_hash) Project Structure Backend-FastAPI-Docker_Build-Pipeline/ ├── backend/ # FastAPI Application │ ├── app/ │ │ ├── api/ │ │ │ ├── routers/ # API endpoints (auth, products, orders) │ │ │ ├── deps.py # Dependency injection │ │ │ └── middleware.py # Correlation ID middleware │ │ ├── core/ # Core configuration │ │ │ ├── config.py # Environment variables │ │ │ ├── security.py # JWT, password hashing │ │ │ └── logging.py # Structured logging │ │ ├── models/ # Pydantic models │ │ │ ├── product.py │ │ │ ├── order.py │ │ │ └── user.py │ │ ├── repositories/ # Data access layer │ │ │ ├── ddb_products.py │ │ │ ├── ddb_orders.py │ │ │ └── ddb_users.py │ │ ├── services/ # Business logic layer │ │ │ ├── product_service.py │ │ │ ├── order_service.py │ │ │ └── auth_service.py │ │ ├── utils/ # Utility functions │ │ │ └── time.py │ │ ├── main.py # FastAPI app entry point │ │ └── lambda_handler.py # Lambda handler (Mangum) │ ├── tests/ # Unit tests │ ├── Dockerfile # Lambda container image │ ├── requirements.txt │ ├── semgrep.yml # Security scanning rules │ └── trivyignore.txt # Trivy ignore list ├── infra/ # Terraform Infrastructure │ ├── main.tf # Main orchestration │ ├── variables.tf │ ├── outputs.tf │ ├── providers.tf │ ├── backend.tf │ └── modules/ │ ├── dynamodb/ # DynamoDB tables │ ├── iam/ # IAM roles \u0026amp; policies │ ├── lambda_container/ # Lambda function │ ├── apigw/ # API Gateway HTTP API │ ├── observability/ # CloudWatch \u0026amp; SNS │ └── route53/ # Custom domain (optional) └── pipeline/ # CI/CD Pipeline ├── buildspec-build.yml # Build stage (Semgrep, Docker, Trivy) ├── buildspec-deploy.yml # Deploy stage (Terraform) ├── codepipeline.tf # CodePipeline configuration └── README.md Content Workshop Overview Prerequisites Setting up CI/CD Pipeline Building FastAPI Backend Monitoring \u0026amp; Observability Clean up Resources AWS Services Used Category Services Compute AWS Lambda (Container Image) API API Gateway HTTP API (v2) Database Amazon DynamoDB (3 tables) Security AWS Secrets Manager, IAM CI/CD AWS CodePipeline, CodeBuild, Amazon ECR Monitoring Amazon CloudWatch (Logs, Metrics, Alarms), SNS DNS Amazon Route 53 (optional) IaC Terraform Security Scanning Semgrep (SAST), Trivy (Container Scan) Estimated Time \u0026amp; Cost Item Details Duration 4-5 hours Level Intermediate to Advanced Cost ~$5-10 (if cleaned up after workshop) "},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at [AWS CLOUD] from [Sept 9] to [Dec 30], I had the valuable opportunity to learn, practice, and apply my specialized knowledge in a professional real-world environment.\nTogether with my team, I built a CI/CD pipeline for the project [Secure Serverless for Global Applications]. Through this process, I significantly improved my skills in [Python Programming, Data Analysis, Technical Report Writing, etc.].\nRegarding work ethic, I always strived to complete assigned tasks well, strictly adhered to company regulations, and proactively collaborated with colleagues to ensure the team\u0026rsquo;s overall progress.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, application of knowledge, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Highlights Beyond completing assigned tasks, I achieved several positive results:\nAhead-of-schedule Completion: Completed the module/feature [20] days earlier than expected with high stability. Rapid Technology Adaptation: Quickly mastered [Gitlab] within the short initial period of the internship, despite not having studied it deeply at university. Process Improvement: Proposed an idea to optimize the [CI/CD] workflow, helping to save processing time for the team. Needs Improvement Although I have put in my best effort, I recognize there are areas I need to refine to become more professional:\nComplex Problem-Solving: I need to develop a more multi-dimensional and calm approach when facing difficult bugs or unexpected situations. Professional Communication: I aim to be more confident and concise when presenting technical ideas in professional meetings or before a group. Time Management: I need to learn how to better prioritize tasks during high-pressure phases of the project to minimize stress. Next Steps Based on the lessons learned from this internship, I have built a self-development plan for the near future:\nTechnical Specialization: Continue to dive deep into [AWS Cloud services] to understand core concepts and best coding practices. Application to Capstone Project: Apply professional workflows (such as Agile/Scrum) and source code management standards learned at the company to my upcoming Graduation Thesis. Language Proficiency: Improve my ability to read professional English documentation to update myself with the latest technology trends quickly. "},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives (per CloudJourney roadmap) Database migration: Schema conversion and data transfer with DMS (Module 2). Hybrid networking: VPN/Direct Connect setup. Post-migration tasks: Testing, optimization, cleanup. Risk mitigation strategies. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 2 — DB migration: Study DMS endpoints, replication instances, ongoing replication. 13/10/2025 13/10/2025 https://cloudjourney.awsstudygroup.com/ - Migrate to AWS Tuesday Labs: Migrate MySQL to RDS PostgreSQL using DMS, validate data integrity. 14/10/2025 14/10/2025 https://cloudjourney.awsstudygroup.com/ - Module 2 Labs Wednesday Hybrid networking: Create Client VPN endpoint, connect EC2 to \u0026ldquo;on-prem\u0026rdquo; VPC. 15/10/2025 15/10/2025 https://cloudjourney.awsstudygroup.com/ - Migrate to AWS Thursday Post-migration: Performance tuning, security scans, application testing. 16/10/2025 16/10/2025 https://cloudjourney.awsstudygroup.com/ - Module 2 Friday Risk management: Document rollback plans, simulate failures, review compliance. 17/10/2025 17/10/2025 https://cloudjourney.awsstudygroup.com/ - Risk Mitigation Results \u0026amp; Achievements (Week 6) DB migrated seamlessly: DMS full load + CDC from on-prem MySQL to RDS, zero data loss verified. Hybrid connectivity established: VPN allowing secure access between VPCs, latency under 50ms. Post-migration optimized: Tuned RDS parameters, scanned for vulnerabilities with Inspector. Risk framework built: Rollback playbook created, failure scenarios tested with Chaos Engineering basics. Module 2 foundational migration complete, ready for optimization. Issues Encountered \u0026amp; Mitigations DMS schema conversion errors → used SCT tool for pre-conversion fixes. VPN auth failures → enabled SAML integration for user access. Post-migration query slowdown → indexed tables and enabled query caching. Next Steps (Week 7) Enter Module 3: Focus on cost optimization and performance efficiency. Implement auto-scaling and right-sizing. Begin security pillar with GuardDuty. Analyze overall journey costs so far. "},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/5-workshop/5.6-cleanup/","title":"Clean Up Resources","tags":[],"description":"","content":"Clean Up Resources Clean up all AWS resources created during the workshop to avoid unnecessary costs.\nQuick Cleanup The easiest way is using Terraform:\ncd Backend-FastAPI-Docker_Build-Pipeline/infra terraform destroy This destroys:\nLambda function API Gateway HTTP API DynamoDB tables IAM roles CloudWatch log groups SNS topics CloudWatch alarms Manual Cleanup Steps If Terraform destroy doesn\u0026rsquo;t work or you need to clean up manually:\n1. Delete ECR Repository # Delete all images first aws ecr list-images \\ --repository-name fastapi-lambda \\ --region ap-southeast-1 \\ --query \u0026#39;imageIds[*]\u0026#39; \\ --output json \u0026gt; image-ids.json aws ecr batch-delete-image \\ --repository-name fastapi-lambda \\ --image-ids file://image-ids.json \\ --region ap-southeast-1 # Delete repository aws ecr delete-repository \\ --repository-name fastapi-lambda \\ --force \\ --region ap-southeast-1 2. Delete Secrets Manager Secret aws secretsmanager delete-secret \\ --secret-id fastapi-jwt-secret \\ --force-delete-without-recovery \\ --region ap-southeast-1 3. Delete CloudWatch Log Groups aws logs delete-log-group \\ --log-group-name /aws/lambda/fastapi-lambda-fn \\ --region ap-southeast-1 4. Delete SNS Subscriptions # List subscriptions aws sns list-subscriptions-by-topic \\ --topic-arn arn:aws:sns:ap-southeast-1:123456789012:fastapi-lambda-alerts \\ --region ap-southeast-1 # Unsubscribe (replace with actual subscription ARN) aws sns unsubscribe \\ --subscription-arn arn:aws:sns:ap-southeast-1:123456789012:fastapi-lambda-alerts:subscription-id \\ --region ap-southeast-1 Verify Deletion Check Lambda Functions aws lambda list-functions \\ --region ap-southeast-1 \\ --query \u0026#39;Functions[?contains(FunctionName, `fastapi`)].FunctionName\u0026#39; Should return empty.\nCheck API Gateway aws apigatewayv2 get-apis \\ --region ap-southeast-1 \\ --query \u0026#39;Items[?contains(Name, `fastapi`)].Name\u0026#39; Check DynamoDB Tables aws dynamodb list-tables \\ --region ap-southeast-1 \\ --query \u0026#39;TableNames[?contains(@, `products`) || contains(@, `orders`) || contains(@, `users`)]\u0026#39; Check ECR Repositories aws ecr describe-repositories \\ --region ap-southeast-1 \\ --query \u0026#39;repositories[?contains(repositoryName, `fastapi`)].repositoryName\u0026#39; Complete Cleanup Script Create cleanup.sh:\n#!/bin/bash REGION=\u0026#34;ap-southeast-1\u0026#34; echo \u0026#34;Destroying Terraform infrastructure...\u0026#34; cd Backend-FastAPI-Docker_Build-Pipeline/infra terraform destroy -auto-approve echo \u0026#34;Deleting ECR repository...\u0026#34; aws ecr delete-repository \\ --repository-name fastapi-lambda \\ --force \\ --region $REGION 2\u0026gt;/dev/null || true echo \u0026#34;Deleting Secrets Manager secret...\u0026#34; aws secretsmanager delete-secret \\ --secret-id fastapi-jwt-secret \\ --force-delete-without-recovery \\ --region $REGION 2\u0026gt;/dev/null || true echo \u0026#34;Deleting CloudWatch log group...\u0026#34; aws logs delete-log-group \\ --log-group-name /aws/lambda/fastapi-lambda-fn \\ --region $REGION 2\u0026gt;/dev/null || true echo \u0026#34;Cleanup completed!\u0026#34; Run:\nchmod +x cleanup.sh ./cleanup.sh Cost Verification After cleanup:\nNavigate to AWS Cost Explorer Check recent charges Verify no resources are running Expected costs after cleanup: $0 (except free tier usage)\nCommon Issues Issue: Terraform destroy fails Solution: Delete resources manually in order, or use terraform destroy -target for specific resources.\nIssue: ECR repository not empty Solution: Delete all images first, then delete repository.\nIssue: DynamoDB table deletion takes time Solution: Wait for deletion to complete. Check status with aws dynamodb describe-table.\nBest Practices Always use Terraform destroy when possible Verify deletion to ensure no orphaned resources Set up billing alerts to monitor unexpected charges Use tags to identify workshop resources Document manual steps for resources not managed by Terraform Make sure to delete all resources to avoid incurring costs. DynamoDB tables and Lambda functions can accumulate charges if left running.\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives (per CloudJourney roadmap) Cost optimization: Right-sizing, Compute Optimizer (Module 3). Performance efficiency: Auto-scaling, caching refinements. Initial security: GuardDuty threat detection. Operational excellence basics: Automation with SSM. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 3 — Cost opt: Analyze usage with Cost Explorer, recommendations from Compute Optimizer. 20/10/2025 20/10/2025 https://cloudjourney.awsstudygroup.com/ - Optimize Tuesday Labs: Right-size EC2 instances, purchase Savings Plans, set up budgets. 21/10/2025 21/10/2025 https://cloudjourney.awsstudygroup.com/ - Module 3 Labs Wednesday Performance: Refine ASG policies, implement ElastiCache for app caching. 22/10/2025 22/10/2025 https://cloudjourney.awsstudygroup.com/ - Optimize Thursday Security intro: Enable GuardDuty, review findings, integrate with EventBridge. 23/10/2025 23/10/2025 https://cloudjourney.awsstudygroup.com/ - Module 3 Friday Automation: Use SSM for patching EC2 fleet, document runbooks. 24/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ - Operational Excellence Results \u0026amp; Achievements (Week 7) Costs reduced by 25%: EC2 right-sized, Savings Plans applied, anomaly detection alerts set. Performance improved: ASG responding in \u0026lt;1min, caching hitting 80% success rate. Threat detection active: GuardDuty scanning logs, simulated threats triaged. Automation streamlined: SSM automating weekly patches, runbooks in Git repo. Mid-journey cost analysis: Total spend tracked, optimizations yielding savings. Issues Encountered \u0026amp; Mitigations Compute Optimizer data lag → waited 48h for full recommendations. GuardDuty false positives → tuned suppression rules based on environment. SSM execution failures → granted additional IAM roles to instances. Next Steps (Week 8) Continue Module 3: Reliability with multi-AZ/DR, deeper security. Fault tolerance patterns. Prepare for modernization in Week 9. Team sharing session on optimizations. "},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" âš ï¸ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I donâ€™t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives (per CloudJourney roadmap) Reliability pillar: High availability, disaster recovery (Module 3). Advanced security: Zero-trust, compliance with Config. Incident response basics. Cross-pillar integration. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 3 — Reliability: Design multi-AZ architectures, RTO/RPO planning. 27/10/2025 27/10/2025 https://cloudjourney.awsstudygroup.com/ - Optimize Tuesday Labs: Set up DR with Pilot Light strategy, test failover to secondary region. 28/10/2025 28/10/2025 https://cloudjourney.awsstudygroup.com/ - Module 3 Labs Wednesday Security advanced: Implement zero-trust with IAM policies, enable AWS Config rules. 29/10/2025 29/10/2025 https://cloudjourney.awsstudygroup.com/ - Optimize Thursday Compliance: Audit resources with Config, remediate non-compliant items. 30/10/2025 30/10/2025 https://cloudjourney.awsstudygroup.com/ - Module 3 Friday Incident response: Simulate outage, use Incident Manager, document lessons. 31/10/2025 31/10/2025 https://cloudjourney.awsstudygroup.com/ - Operational Excellence Results \u0026amp; Achievements (Week 8) HA/DR resilient: Multi-AZ RDS/ECS clusters, Pilot Light DR tested with \u0026lt;15min RTO. Zero-trust enforced: Least-privilege IAM, Config conformance packs applied. Compliance score 95%: Automated remediation for common rules like encryption. IR playbook refined: Outage simulation resolved in 10min, integrated with PagerDuty. Optimizations integrated: Security/reliability enhancing prior workloads. Issues Encountered \u0026amp; Mitigations DR cross-region latency → optimized with global Accelerator. Config rule evaluation delays → increased evaluation frequency. IR simulation alert fatigue → refined notification thresholds. Next Steps (Week 9) Transition to Module 4: Microservices decomposition. Serverless patterns with Lambda/EventBridge. API design with API Gateway/AppSync. Modernization team demo. Note: All labs and content follow the CloudJourney curriculum at https://cloudjourney.awsstudygroup.com/. Ensure screenshots, logs, and scripts are stored in the project repository for verification.\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives (per CloudJourney roadmap) Microservices: Bounded contexts, decomposition of monolith (Module 4). Serverless: Event-driven with Lambda, Step Functions. API-first: REST/GraphQL design. DevOps integration. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 4 — Microservices: Study domain-driven design, decompose sample app into services. 03/11/2025 03/11/2025 https://cloudjourney.awsstudygroup.com/ - Modernize Tuesday Labs: Refactor monolith to ECS microservices, use API Gateway for routing. 04/11/2025 04/11/2025 https://cloudjourney.awsstudygroup.com/ - Module 4 Labs Wednesday Serverless: Build workflow with Lambda and EventBridge rules. 05/11/2025 05/11/2025 https://cloudjourney.awsstudygroup.com/ - Modernize Thursday APIs: Create REST API with Gateway, GraphQL with AppSync. 06/11/2025 06/11/2025 https://cloudjourney.awsstudygroup.com/ - Module 4 Friday DevOps: Set up CI/CD with CodePipeline for microservices deployment. 07/11/2025 07/11/2025 https://cloudjourney.awsstudygroup.com/ - DevOps Results \u0026amp; Achievements (Week 9) Monolith decomposed: 3 bounded services running independently on ECS, inter-service comms via SQS. Event-driven app: Lambda orchestrating order processing, triggered by EventBridge. APIs live: REST endpoints secured with Cognito, GraphQL schema with resolvers. CI/CD automated: CodePipeline deploying updates, integrated with GitHub. Modernization demo: End-to-end refactored app showcased to team. Issues Encountered \u0026amp; Mitigations Service discovery challenges → integrated Service Discovery with ECS. EventBridge rule matching issues → debugged with CloudWatch Logs Insights. Pipeline approval gates slow → automated for non-prod environments. Next Steps (Week 10) Module 5: Container orchestration with ECS/EKS. Fargate for serverless containers. Service mesh with App Mesh. Security in containers. Note: All labs and content follow the CloudJourney curriculum at https://cloudjourney.awsstudygroup.com/. Ensure screenshots, logs, and scripts are stored in the project repository for verification.\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives (per CloudJourney roadmap) Container orchestration: Deep dive into ECS and EKS (Module 5). Serverless containers with Fargate. Networking and scaling in Kubernetes. Container security best practices. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 5 — ECS advanced: Cluster networking, service discovery, blue-green deployments. 10/11/2025 10/11/2025 https://cloudjourney.awsstudygroup.com/ - Containers Tuesday Labs: Deploy multi-container app on ECS, integrate with ALB and RDS. 11/11/2025 11/11/2025 https://cloudjourney.awsstudygroup.com/ - Module 5 Labs Wednesday EKS intro: Set up managed node groups, deploy Helm charts. 12/11/2025 12/11/2025 https://cloudjourney.awsstudygroup.com/ - Containers Thursday Fargate: Run EKS pods on Fargate, test scaling. 13/11/2025 13/11/2025 https://cloudjourney.awsstudygroup.com/ - Module 5 Friday Security: Image scanning with ECR, runtime protection with GuardDuty. 14/11/2025 14/11/2025 https://cloudjourney.awsstudygroup.com/ - Container Security Results \u0026amp; Achievements (Week 10) ECS production-ready: Blue-green deployment with zero downtime, service mesh via App Mesh. EKS cluster operational: Kubernetes workloads deployed, autoscaling HPA configured. Fargate efficiency: Serverless pods reducing management overhead, cost 20% lower. Secure pipeline: Scanned images blocking vulnerabilities, GuardDuty alerts for anomalies. Containerized modernization: Previous microservices migrated to EKS. Issues Encountered \u0026amp; Mitigations EKS pod scheduling failures → adjusted node taints and affinities. Fargate networking limits → used VPC CNI for pod IPs. Scanning false negatives → updated ECR policies for latest scans. Next Steps (Week 11) Module 6: Data lake with S3/Lake Formation. ETL pipelines with Glue. Real-time analytics with Kinesis. Governance and visualization. Note: All labs and content follow the CloudJourney curriculum at https://cloudjourney.awsstudygroup.com/. Ensure screenshots, logs, and scripts are stored in the project repository for verification.\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives (per CloudJourney roadmap) Data lake architecture: S3 partitioning, Lake Formation governance (Module 6). ETL/ELT: Glue jobs for data processing. Streaming: Kinesis for real-time ingestion. BI basics with QuickSight. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 6 — Data lake: Design S3-based lake, enable versioning, set up Lake Formation catalogs. 17/11/2025 17/11/2025 https://cloudjourney.awsstudygroup.com/ - Data \u0026amp; Analytics Tuesday Labs: Partition data in S3, grant permissions via Lake Formation. 18/11/2025 18/11/2025 https://cloudjourney.awsstudygroup.com/ - Module 6 Labs Wednesday ETL: Create Glue crawlers and jobs, transform CSV to Parquet. 19/11/2025 19/11/2025 https://cloudjourney.awsstudygroup.com/ - Data \u0026amp; Analytics Thursday Streaming: Set up Kinesis Data Streams, produce/consume events. 20/11/2025 20/11/2025 https://cloudjourney.awsstudygroup.com/ - Module 6 Friday Visualization: Connect QuickSight to S3/Glue, build dashboards. 21/11/2025 21/11/2025 https://cloudjourney.awsstudygroup.com/ - BI Results \u0026amp; Achievements (Week 11) Data lake built: S3 organized with Athena queries, Lake Formation securing access. ETL automated: Glue jobs processing 1M rows, output to S3 optimized formats. Real-time pipeline: Kinesis streaming app logs, analytics with Kinesis Analytics. Dashboards interactive: QuickSight visualizing ETL outputs, shared with team. Data journey complete: From ingestion to insights, integrated with prior services. Issues Encountered \u0026amp; Mitigations Lake Formation permission propagation delays → used blueprints for faster setup. Glue job memory errors → scaled DPU allocation. Kinesis shard limits → monitored throughput and resharded. Next Steps (Week 12) Module 7: Intro to AI/ML with Bedrock, SageMaker basics. Generative AI applications. Capstone project review. Final reflections and workforce program prep. Note: All labs and content follow the CloudJourney curriculum at https://cloudjourney.awsstudygroup.com/. Ensure screenshots, logs, and scripts are stored in the project repository for verification.\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives (per CloudJourney roadmap) AI/ML foundations: Bedrock for foundation models, SageMaker for custom ML (Module 7). Build simple RAG app. Responsible AI practices. Capstone integration and review. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 7 — AI intro: Explore Bedrock models, invoke LLMs for text generation. 24/11/2025 24/11/2025 https://cloudjourney.awsstudygroup.com/ - AI/ML Tuesday Labs: Build RAG pipeline with Bedrock and Kendra for document Q\u0026amp;A. 25/11/2025 25/11/2025 https://cloudjourney.awsstudygroup.com/ - Module 7 Labs Wednesday Custom ML: SageMaker Studio setup, train simple model on S3 data. 26/11/2025 26/11/2025 https://cloudjourney.awsstudygroup.com/ - AI/ML Thursday Responsible AI: Bias detection with Clarify, explainability tools. 27/11/2025 27/11/2025 https://cloudjourney.awsstudygroup.com/ - Module 7 Friday Capstone: Integrate AI into data pipeline, full journey review, prepare submission. 28/11/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ - Review Results \u0026amp; Achievements (Week 12) Generative AI app: Bedrock-powered chatbot querying S3 docs via RAG, accuracy \u0026gt;85%. ML model deployed: SageMaker endpoint serving predictions, integrated with Lambda. Ethical practices applied: Models audited for bias, explanations added to outputs. Capstone delivered: End-to-end app combining migration, optimization, containers, data, AI. 12-week journey complete: All modules covered, portfolio built, ready for FCJ Workforce. Issues Encountered \u0026amp; Mitigations Bedrock token limits → chunked inputs for longer queries. SageMaker training costs → used spot instances for non-urgent jobs. Bias in dataset → augmented training data for balance. Next Steps (Post-Program) Apply to FCJ Workforce Program. Contribute to community projects. Pursue AWS certifications (Solutions Architect Associate). Mentor next cohort on learnings. Note: All labs and content follow the CloudJourney curriculum at https://cloudjourney.awsstudygroup.com/. Ensure screenshots, logs, and scripts are stored in the project repository for verification.\n"},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://huyiaaa.github.io/aws_report_fcj/en/tags/","title":"Tags","tags":[],"description":"","content":""}]